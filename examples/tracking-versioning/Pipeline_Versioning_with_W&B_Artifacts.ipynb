{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V-XlJUMvuKMH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac7YlczouKMH"
      },
      "source": [
        "# 1Ô∏è‚É£ Log a Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p6108wetuKMI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tung.dao/miniconda3/envs/soict/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import random \n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import TensorDataset\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data parameters\n",
        "num_classes = 10\n",
        "input_shape = (1, 28, 28)\n",
        "\n",
        "# drop slow mirror from list of MNIST mirrors\n",
        "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
        "                                      if not mirror.startswith(\"http://yann.lecun.com\")]\n",
        "\n",
        "def load(train_size=50_000):\n",
        "    \"\"\"\n",
        "    # Load the data\n",
        "    \"\"\"\n",
        "\n",
        "    # the data, split between train and test sets\n",
        "    train = torchvision.datasets.MNIST(\"./\", train=True, download=True)\n",
        "    test = torchvision.datasets.MNIST(\"./\", train=False, download=True)\n",
        "    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)\n",
        "\n",
        "    # split off a validation set for hyperparameter tuning\n",
        "    x_train, x_val = x_train[:train_size], x_train[train_size:]\n",
        "    y_train, y_val = y_train[:train_size], y_train[train_size:]\n",
        "\n",
        "    training_set = TensorDataset(x_train, y_train)\n",
        "    validation_set = TensorDataset(x_val, y_val)\n",
        "    test_set = TensorDataset(x_test, y_test)\n",
        "\n",
        "    datasets = [training_set, validation_set, test_set]\n",
        "\n",
        "    return datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qzEwTi7HuKMJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdemo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/examples/wandb/run-20221029_125117-exrxinmj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"http://localhost:8080/demo/artifacts-example/runs/exrxinmj\" target=\"_blank\">solar-lion-1</a></strong> to <a href=\"http://localhost:8080/demo/artifacts-example\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9912422/9912422 [00:06<00:00, 1542257.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28881/28881 [00:00<00:00, 93412.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1648877/1648877 [00:02<00:00, 802880.69it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4542/4542 [00:00<00:00, 1316553.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">solar-lion-1</strong>: <a href=\"http://localhost:8080/demo/artifacts-example/runs/exrxinmj\" target=\"_blank\">http://localhost:8080/demo/artifacts-example/runs/exrxinmj</a><br/>Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221029_125117-exrxinmj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def load_and_log():\n",
        "\n",
        "    # üöÄ start a run, with a type to label it and a project it can call home\n",
        "    with wandb.init(project=\"artifacts-example\", job_type=\"load-data\") as run:\n",
        "        \n",
        "        datasets = load()  # separate code for loading the datasets\n",
        "        names = [\"training\", \"validation\", \"test\"]\n",
        "\n",
        "        # üè∫ create our Artifact\n",
        "        raw_data = wandb.Artifact(\n",
        "            \"mnist-raw\", type=\"dataset\",\n",
        "            description=\"Raw MNIST dataset, split into train/val/test\",\n",
        "            metadata={\"source\": \"torchvision.datasets.MNIST\",\n",
        "                      \"sizes\": [len(dataset) for dataset in datasets]})\n",
        "\n",
        "        for name, data in zip(names, datasets):\n",
        "            # üê£ Store a new file in the artifact, and write something into its contents.\n",
        "            with raw_data.new_file(name + \".pt\", mode=\"wb\") as file:\n",
        "                x, y = data.tensors\n",
        "                torch.save((x, y), file)\n",
        "\n",
        "        # ‚úçÔ∏è Save the artifact to W&B.\n",
        "        run.log_artifact(raw_data)\n",
        "\n",
        "load_and_log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36mbGhmsuKML"
      },
      "source": [
        "# 2Ô∏è‚É£ Use a Logged Dataset Artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uPOlzJFpuKML"
      },
      "outputs": [],
      "source": [
        "def preprocess(dataset, normalize=True, expand_dims=True):\n",
        "    \"\"\"\n",
        "    ## Prepare the data\n",
        "    \"\"\"\n",
        "    x, y = dataset.tensors\n",
        "\n",
        "    if normalize:\n",
        "        # Scale images to the [0, 1] range\n",
        "        x = x.type(torch.float32) / 255\n",
        "\n",
        "    if expand_dims:\n",
        "        # Make sure images have shape (1, 28, 28)\n",
        "        x = torch.unsqueeze(x, 1)\n",
        "    \n",
        "    return TensorDataset(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uB-pDTrpuKMM"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_log(steps):\n",
        "\n",
        "    with wandb.init(project=\"artifacts-example\", job_type=\"preprocess-data\") as run:\n",
        "\n",
        "        processed_data = wandb.Artifact(\n",
        "            \"mnist-preprocess\", type=\"dataset\",\n",
        "            description=\"Preprocessed MNIST dataset\",\n",
        "            metadata=steps)\n",
        "         \n",
        "        # ‚úîÔ∏è declare which artifact we'll be using\n",
        "        raw_data_artifact = run.use_artifact('mnist-raw:latest')\n",
        "\n",
        "        # üì• if need be, download the artifact\n",
        "        raw_dataset = raw_data_artifact.download()\n",
        "        \n",
        "        for split in [\"training\", \"validation\", \"test\"]:\n",
        "            raw_split = read(raw_dataset, split)\n",
        "            processed_dataset = preprocess(raw_split, **steps)\n",
        "\n",
        "            with processed_data.new_file(split + \".pt\", mode=\"wb\") as file:\n",
        "                x, y = processed_dataset.tensors\n",
        "                torch.save((x, y), file)\n",
        "\n",
        "        run.log_artifact(processed_data)\n",
        "\n",
        "\n",
        "def read(data_dir, split):\n",
        "    filename = split + \".pt\"\n",
        "    x, y = torch.load(os.path.join(data_dir, filename))\n",
        "\n",
        "    return TensorDataset(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zEKd-ypAuKMM"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/examples/wandb/run-20221029_125200-jspxo1ml</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"http://localhost:8080/demo/artifacts-example/runs/jspxo1ml\" target=\"_blank\">golden-hill-2</a></strong> to <a href=\"http://localhost:8080/demo/artifacts-example\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact mnist-raw:latest, 98.19MB. 3 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
            "Done. 0:0:0.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">golden-hill-2</strong>: <a href=\"http://localhost:8080/demo/artifacts-example/runs/jspxo1ml\" target=\"_blank\">http://localhost:8080/demo/artifacts-example/runs/jspxo1ml</a><br/>Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221029_125200-jspxo1ml/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "steps = {\"normalize\": True,\n",
        "         \"expand_dims\": True}\n",
        "\n",
        "preprocess_and_log(steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGZ_RySxuKMN"
      },
      "source": [
        "# 3Ô∏è‚É£ Log a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NIQSRG0DuKMN"
      },
      "outputs": [],
      "source": [
        "from math import floor\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, hidden_layer_sizes=[32, 64],\n",
        "                  kernel_sizes=[3],\n",
        "                  activation=\"ReLU\",\n",
        "                  pool_sizes=[2],\n",
        "                  dropout=0.5,\n",
        "                  num_classes=num_classes,\n",
        "                  input_shape=input_shape):\n",
        "      \n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "              nn.Conv2d(in_channels=input_shape[0], out_channels=hidden_layer_sizes[0], kernel_size=kernel_sizes[0]),\n",
        "              getattr(nn, activation)(),\n",
        "              nn.MaxPool2d(kernel_size=pool_sizes[0])\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "              nn.Conv2d(in_channels=hidden_layer_sizes[0], out_channels=hidden_layer_sizes[-1], kernel_size=kernel_sizes[-1]),\n",
        "              getattr(nn, activation)(),\n",
        "              nn.MaxPool2d(kernel_size=pool_sizes[-1])\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "              nn.Flatten(),\n",
        "              nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        fc_input_dims = floor((input_shape[1] - kernel_sizes[0] + 1) / pool_sizes[0]) # layer 1 output size\n",
        "        fc_input_dims = floor((fc_input_dims - kernel_sizes[-1] + 1) / pool_sizes[-1]) # layer 2 output size\n",
        "        fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-1] # layer 3 output size\n",
        "\n",
        "        self.fc = nn.Linear(fc_input_dims, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c-0RqDeUuKMO"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/examples/wandb/run-20221029_125230-1fbxon64</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"http://localhost:8080/demo/artifacts-example/runs/1fbxon64\" target=\"_blank\">apricot-vortex-3</a></strong> to <a href=\"http://localhost:8080/demo/artifacts-example\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">apricot-vortex-3</strong>: <a href=\"http://localhost:8080/demo/artifacts-example/runs/1fbxon64\" target=\"_blank\">http://localhost:8080/demo/artifacts-example/runs/1fbxon64</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 2 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221029_125230-1fbxon64/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def build_model_and_log(config):\n",
        "    with wandb.init(project=\"artifacts-example\", job_type=\"initialize\", config=config) as run:\n",
        "        config = wandb.config\n",
        "        \n",
        "        model = ConvNet(**config)\n",
        "\n",
        "        model_artifact = wandb.Artifact(\n",
        "            \"convnet\", type=\"model\",\n",
        "            description=\"Simple AlexNet style CNN\",\n",
        "            metadata=dict(config))\n",
        "\n",
        "        torch.save(model.state_dict(), \"initialized_model.pth\")\n",
        "        # ‚ûï another way to add a file to an Artifact\n",
        "        model_artifact.add_file(\"initialized_model.pth\")\n",
        "\n",
        "        wandb.save(\"initialized_model.pth\")\n",
        "\n",
        "        run.log_artifact(model_artifact)\n",
        "\n",
        "model_config = {\"hidden_layer_sizes\": [32, 64],\n",
        "                \"kernel_sizes\": [3],\n",
        "                \"activation\": \"ReLU\",\n",
        "                \"pool_sizes\": [2],\n",
        "                \"dropout\": 0.5,\n",
        "                \"num_classes\": 10}\n",
        "\n",
        "build_model_and_log(model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AdlqnzIuKMO"
      },
      "source": [
        "# 4Ô∏è‚É£ Use a Logged Model Artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X_GTAeIDuKMO"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train(model, train_loader, valid_loader, config):\n",
        "    optimizer = getattr(torch.optim, config.optimizer)(model.parameters())\n",
        "    model.train()\n",
        "    example_ct = 0\n",
        "    for epoch in range(config.epochs):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            example_ct += len(data)\n",
        "\n",
        "            if batch_idx % config.batch_log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0%})]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    batch_idx / len(train_loader), loss.item()))\n",
        "                \n",
        "                train_log(loss, example_ct, epoch)\n",
        "\n",
        "        # evaluate the model on the validation set at each epoch\n",
        "        loss, accuracy = test(model, valid_loader)  \n",
        "        test_log(loss, accuracy, example_ct, epoch)\n",
        "\n",
        "    \n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum')  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    \n",
        "    return test_loss, accuracy\n",
        "\n",
        "\n",
        "def train_log(loss, example_ct, epoch):\n",
        "    loss = float(loss)\n",
        "\n",
        "    # where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"train/loss\": loss}, step=example_ct)\n",
        "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
        "    \n",
        "\n",
        "def test_log(loss, accuracy, example_ct, epoch):\n",
        "    loss = float(loss)\n",
        "    accuracy = float(accuracy)\n",
        "\n",
        "    # where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"validation/loss\": loss, \"validation/accuracy\": accuracy}, step=example_ct)\n",
        "    print(f\"Loss/accuracy after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}/{accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "llrI_XL0uKMQ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader):\n",
        "    \"\"\"\n",
        "    ## Evaluate the trained model\n",
        "    \"\"\"\n",
        "\n",
        "    loss, accuracy = test(model, test_loader)\n",
        "    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)\n",
        "\n",
        "    return loss, accuracy, highest_losses, hardest_examples, true_labels, predictions\n",
        "\n",
        "def get_hardest_k_examples(model, testing_set, k=32):\n",
        "    model.eval()\n",
        "\n",
        "    loader = DataLoader(testing_set, 1, shuffle=False)\n",
        "\n",
        "    # get the losses and predictions for each item in the dataset\n",
        "    losses = None\n",
        "    predictions = None\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            \n",
        "            if losses is None:\n",
        "                losses = loss.view((1, 1))\n",
        "                predictions = pred\n",
        "            else:\n",
        "                losses = torch.cat((losses, loss.view((1, 1))), 0)\n",
        "                predictions = torch.cat((predictions, pred), 0)\n",
        "\n",
        "    argsort_loss = torch.argsort(losses, dim=0)\n",
        "\n",
        "    highest_k_losses = losses[argsort_loss[-k:]]\n",
        "    hardest_k_examples = testing_set[argsort_loss[-k:]][0]\n",
        "    true_labels = testing_set[argsort_loss[-k:]][1]\n",
        "    predicted_labels = predictions[argsort_loss[-k:]]\n",
        "\n",
        "    return highest_k_losses, hardest_k_examples, true_labels, predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Xn5E5zSzuKMQ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_and_log(config):\n",
        "\n",
        "    with wandb.init(project=\"artifacts-example\", job_type=\"train\", config=config) as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        data = run.use_artifact('mnist-preprocess:latest')\n",
        "        data_dir = data.download()\n",
        "\n",
        "        training_dataset =  read(data_dir, \"training\")\n",
        "        validation_dataset = read(data_dir, \"validation\")\n",
        "\n",
        "        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)\n",
        "        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)\n",
        "        \n",
        "        model_artifact = run.use_artifact(\"convnet:latest\")\n",
        "        model_dir = model_artifact.download()\n",
        "        model_path = os.path.join(model_dir, \"initialized_model.pth\")\n",
        "        model_config = model_artifact.metadata\n",
        "        config.update(model_config)\n",
        "\n",
        "        model = ConvNet(**model_config)\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        model = model.to(device)\n",
        " \n",
        "        train(model, train_loader, validation_loader, config)\n",
        "\n",
        "        model_artifact = wandb.Artifact(\n",
        "            \"trained-model\", type=\"model\",\n",
        "            description=\"Trained NN model\",\n",
        "            metadata=dict(model_config))\n",
        "\n",
        "        torch.save(model.state_dict(), \"trained_model.pth\")\n",
        "        model_artifact.add_file(\"trained_model.pth\")\n",
        "        wandb.save(\"trained_model.pth\")\n",
        "\n",
        "        run.log_artifact(model_artifact)\n",
        "\n",
        "    return model\n",
        "\n",
        "    \n",
        "def evaluate_and_log(config=None):\n",
        "    \n",
        "    with wandb.init(project=\"artifacts-example\", job_type=\"report\", config=config) as run:\n",
        "        data = run.use_artifact('mnist-preprocess:latest')\n",
        "        data_dir = data.download()\n",
        "        testing_set = read(data_dir, \"test\")\n",
        "\n",
        "        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=128, shuffle=False)\n",
        "\n",
        "        model_artifact = run.use_artifact(\"trained-model:latest\")\n",
        "        model_dir = model_artifact.download()\n",
        "        model_path = os.path.join(model_dir, \"trained_model.pth\")\n",
        "        model_config = model_artifact.metadata\n",
        "\n",
        "        model = ConvNet(**model_config)\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        model.to(device)\n",
        "\n",
        "        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)\n",
        "\n",
        "        run.summary.update({\"loss\": loss, \"accuracy\": accuracy})\n",
        "\n",
        "        wandb.log({\"high-loss-examples\":\n",
        "            [wandb.Image(hard_example, caption=str(int(pred)) + \",\" +  str(int(label)))\n",
        "             for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Dr8SDrkSuKMQ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/examples/wandb/run-20221029_125242-1hqiy2ma</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"http://localhost:8080/demo/artifacts-example/runs/1hqiy2ma\" target=\"_blank\">balmy-energy-4</a></strong> to <a href=\"http://localhost:8080/demo/artifacts-example\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact mnist-preprocess:latest, 210.35MB. 3 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.330907\n",
            "Loss after 00128 examples: 2.331\n",
            "Train Epoch: 0 [3200/50000 (6%)]\tLoss: 1.046478\n",
            "Loss after 03328 examples: 1.046\n",
            "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 0.528573\n",
            "Loss after 06528 examples: 0.529\n",
            "Train Epoch: 0 [9600/50000 (19%)]\tLoss: 0.354153\n",
            "Loss after 09728 examples: 0.354\n",
            "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 0.212520\n",
            "Loss after 12928 examples: 0.213\n",
            "Train Epoch: 0 [16000/50000 (32%)]\tLoss: 0.324148\n",
            "Loss after 16128 examples: 0.324\n",
            "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 0.188879\n",
            "Loss after 19328 examples: 0.189\n",
            "Train Epoch: 0 [22400/50000 (45%)]\tLoss: 0.234382\n",
            "Loss after 22528 examples: 0.234\n",
            "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 0.144055\n",
            "Loss after 25728 examples: 0.144\n",
            "Train Epoch: 0 [28800/50000 (58%)]\tLoss: 0.104811\n",
            "Loss after 28928 examples: 0.105\n",
            "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 0.258159\n",
            "Loss after 32128 examples: 0.258\n",
            "Train Epoch: 0 [35200/50000 (70%)]\tLoss: 0.124250\n",
            "Loss after 35328 examples: 0.124\n",
            "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 0.124537\n",
            "Loss after 38528 examples: 0.125\n",
            "Train Epoch: 0 [41600/50000 (83%)]\tLoss: 0.120809\n",
            "Loss after 41728 examples: 0.121\n",
            "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 0.245159\n",
            "Loss after 44928 examples: 0.245\n",
            "Train Epoch: 0 [48000/50000 (96%)]\tLoss: 0.121275\n",
            "Loss after 48128 examples: 0.121\n",
            "Loss/accuracy after 50000 examples: 0.095/97.440\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.127566\n",
            "Loss after 50128 examples: 0.128\n",
            "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 0.068296\n",
            "Loss after 53328 examples: 0.068\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.109071\n",
            "Loss after 56528 examples: 0.109\n",
            "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 0.093964\n",
            "Loss after 59728 examples: 0.094\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.057987\n",
            "Loss after 62928 examples: 0.058\n",
            "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 0.096420\n",
            "Loss after 66128 examples: 0.096\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.070373\n",
            "Loss after 69328 examples: 0.070\n",
            "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 0.063037\n",
            "Loss after 72528 examples: 0.063\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.074745\n",
            "Loss after 75728 examples: 0.075\n",
            "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 0.034281\n",
            "Loss after 78928 examples: 0.034\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.123505\n",
            "Loss after 82128 examples: 0.124\n",
            "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 0.076524\n",
            "Loss after 85328 examples: 0.077\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.088081\n",
            "Loss after 88528 examples: 0.088\n",
            "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 0.047089\n",
            "Loss after 91728 examples: 0.047\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.105628\n",
            "Loss after 94928 examples: 0.106\n",
            "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 0.053115\n",
            "Loss after 98128 examples: 0.053\n",
            "Loss/accuracy after 100000 examples: 0.064/98.200\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.072981\n",
            "Loss after 100128 examples: 0.073\n",
            "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 0.027937\n",
            "Loss after 103328 examples: 0.028\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.100334\n",
            "Loss after 106528 examples: 0.100\n",
            "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 0.048616\n",
            "Loss after 109728 examples: 0.049\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.044867\n",
            "Loss after 112928 examples: 0.045\n",
            "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 0.052198\n",
            "Loss after 116128 examples: 0.052\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.043460\n",
            "Loss after 119328 examples: 0.043\n",
            "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 0.037791\n",
            "Loss after 122528 examples: 0.038\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.052733\n",
            "Loss after 125728 examples: 0.053\n",
            "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 0.019288\n",
            "Loss after 128928 examples: 0.019\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.087647\n",
            "Loss after 132128 examples: 0.088\n",
            "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 0.064744\n",
            "Loss after 135328 examples: 0.065\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.084050\n",
            "Loss after 138528 examples: 0.084\n",
            "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 0.037981\n",
            "Loss after 141728 examples: 0.038\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.062478\n",
            "Loss after 144928 examples: 0.062\n",
            "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 0.041256\n",
            "Loss after 148128 examples: 0.041\n",
            "Loss/accuracy after 150000 examples: 0.056/98.410\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.053486\n",
            "Loss after 150128 examples: 0.053\n",
            "Train Epoch: 3 [3200/50000 (6%)]\tLoss: 0.019220\n",
            "Loss after 153328 examples: 0.019\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.094717\n",
            "Loss after 156528 examples: 0.095\n",
            "Train Epoch: 3 [9600/50000 (19%)]\tLoss: 0.026964\n",
            "Loss after 159728 examples: 0.027\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.032232\n",
            "Loss after 162928 examples: 0.032\n",
            "Train Epoch: 3 [16000/50000 (32%)]\tLoss: 0.038763\n",
            "Loss after 166128 examples: 0.039\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.030891\n",
            "Loss after 169328 examples: 0.031\n",
            "Train Epoch: 3 [22400/50000 (45%)]\tLoss: 0.024210\n",
            "Loss after 172528 examples: 0.024\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.043378\n",
            "Loss after 175728 examples: 0.043\n",
            "Train Epoch: 3 [28800/50000 (58%)]\tLoss: 0.011446\n",
            "Loss after 178928 examples: 0.011\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.059353\n",
            "Loss after 182128 examples: 0.059\n",
            "Train Epoch: 3 [35200/50000 (70%)]\tLoss: 0.059419\n",
            "Loss after 185328 examples: 0.059\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.078793\n",
            "Loss after 188528 examples: 0.079\n",
            "Train Epoch: 3 [41600/50000 (83%)]\tLoss: 0.032860\n",
            "Loss after 191728 examples: 0.033\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.041527\n",
            "Loss after 194928 examples: 0.042\n",
            "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 0.029481\n",
            "Loss after 198128 examples: 0.029\n",
            "Loss/accuracy after 200000 examples: 0.052/98.490\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.042807\n",
            "Loss after 200128 examples: 0.043\n",
            "Train Epoch: 4 [3200/50000 (6%)]\tLoss: 0.017423\n",
            "Loss after 203328 examples: 0.017\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.090287\n",
            "Loss after 206528 examples: 0.090\n",
            "Train Epoch: 4 [9600/50000 (19%)]\tLoss: 0.017655\n",
            "Loss after 209728 examples: 0.018\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.029416\n",
            "Loss after 212928 examples: 0.029\n",
            "Train Epoch: 4 [16000/50000 (32%)]\tLoss: 0.032689\n",
            "Loss after 216128 examples: 0.033\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.022855\n",
            "Loss after 219328 examples: 0.023\n",
            "Train Epoch: 4 [22400/50000 (45%)]\tLoss: 0.020911\n",
            "Loss after 222528 examples: 0.021\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.040476\n",
            "Loss after 225728 examples: 0.040\n",
            "Train Epoch: 4 [28800/50000 (58%)]\tLoss: 0.008080\n",
            "Loss after 228928 examples: 0.008\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.042937\n",
            "Loss after 232128 examples: 0.043\n",
            "Train Epoch: 4 [35200/50000 (70%)]\tLoss: 0.053568\n",
            "Loss after 235328 examples: 0.054\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.069425\n",
            "Loss after 238528 examples: 0.069\n",
            "Train Epoch: 4 [41600/50000 (83%)]\tLoss: 0.027696\n",
            "Loss after 241728 examples: 0.028\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.030431\n",
            "Loss after 244928 examples: 0.030\n",
            "Train Epoch: 4 [48000/50000 (96%)]\tLoss: 0.019090\n",
            "Loss after 248128 examples: 0.019\n",
            "Loss/accuracy after 250000 examples: 0.049/98.640\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>validation/accuracy</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà</td></tr><tr><td>validation/loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>0.01909</td></tr><tr><td>validation/accuracy</td><td>98.64</td></tr><tr><td>validation/loss</td><td>0.04874</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">balmy-energy-4</strong>: <a href=\"http://localhost:8080/demo/artifacts-example/runs/1hqiy2ma\" target=\"_blank\">http://localhost:8080/demo/artifacts-example/runs/1hqiy2ma</a><br/>Synced 7 W&B file(s), 0 media file(s), 1 artifact file(s) and 2 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221029_125242-1hqiy2ma/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/examples/wandb/run-20221029_125404-1gplls0b</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"http://localhost:8080/demo/artifacts-example/runs/1gplls0b\" target=\"_blank\">swept-moon-5</a></strong> to <a href=\"http://localhost:8080/demo/artifacts-example\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact mnist-preprocess:latest, 210.35MB. 3 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>98.83</td></tr><tr><td>loss</td><td>0.03666</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">swept-moon-5</strong>: <a href=\"http://localhost:8080/demo/artifacts-example/runs/1gplls0b\" target=\"_blank\">http://localhost:8080/demo/artifacts-example/runs/1gplls0b</a><br/>Synced 7 W&B file(s), 32 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221029_125404-1gplls0b/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_config = {\"batch_size\": 128,\n",
        "                \"epochs\": 5,\n",
        "                \"batch_log_interval\": 25,\n",
        "                \"optimizer\": \"Adam\"}\n",
        "\n",
        "model = train_and_log(train_config)\n",
        "evaluate_and_log()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('soict')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "291bacc89d47c570d681c8cb8133df5aeea2ebc003da29abf8c072691acbbc14"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
