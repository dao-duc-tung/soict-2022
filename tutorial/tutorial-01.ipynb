{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "1. Data versioning\n",
    "1. Experiment tracking\n",
    "1. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"soict-2022\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Log a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdemo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221031_094422-1c07696l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/1c07696l\" target=\"_blank\">unearthly-wand-1</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">unearthly-wand-1</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/1c07696l\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/1c07696l</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221031_094422-1c07696l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data parameters\n",
    "num_classes = 10\n",
    "input_shape = (1, 28, 28)\n",
    "n_train_valid = 1000\n",
    "n_test = 200\n",
    "\n",
    "# drop slow mirror from list of MNIST mirrors\n",
    "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
    "                                        if not mirror.startswith(\"http://yann.lecun.com\")]\n",
    "\n",
    "def load():\n",
    "    # split between train and test sets\n",
    "    train = torchvision.datasets.MNIST(\"./\", train=True, download=True)\n",
    "    test = torchvision.datasets.MNIST(\"./\", train=False, download=True)\n",
    "    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)\n",
    "    x_train = x_train[:n_train_valid]\n",
    "    y_train = y_train[:n_train_valid]\n",
    "    x_test = x_test[:n_test]\n",
    "    y_test = y_test[:n_test]\n",
    "\n",
    "    # split off a validation set for hyperparameter tuning\n",
    "    train_size = int(n_train_valid * 0.75)\n",
    "    x_train, x_val = x_train[:train_size], x_train[train_size:]\n",
    "    y_train, y_val = y_train[:train_size], y_train[train_size:]\n",
    "\n",
    "    training_set = TensorDataset(x_train, y_train)\n",
    "    validation_set = TensorDataset(x_val, y_val)\n",
    "    test_set = TensorDataset(x_test, y_test)\n",
    "    datasets = [training_set, validation_set, test_set]\n",
    "    return datasets\n",
    "\n",
    "def load_and_log():\n",
    "    # start a run, with a type to label it and a project name\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"load-data\") as run:\n",
    "        datasets = load()  # separate code for loading the datasets\n",
    "        names = [\"training\", \"validation\", \"test\"]\n",
    "\n",
    "        # create our Artifact\n",
    "        raw_data = wandb.Artifact(\n",
    "            \"mnist-raw\", type=\"dataset\",\n",
    "            description=\"Raw MNIST dataset, split into train/val/test\",\n",
    "            metadata={\"source\": \"torchvision.datasets.MNIST\",\n",
    "                        \"sizes\": [len(dataset) for dataset in datasets]})\n",
    "\n",
    "        for name, data in zip(names, datasets):\n",
    "            # Store a new file in the artifact, and write data\n",
    "            with raw_data.new_file(name + \".pt\", mode=\"wb\") as file:\n",
    "                x, y = data.tensors\n",
    "                torch.save((x, y), file)\n",
    "\n",
    "        # Save the artifact to W&B.\n",
    "        run.log_artifact(raw_data)\n",
    "\n",
    "load_and_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Preprocess a logged dataset artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221031_094432-3nn1toac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/3nn1toac\" target=\"_blank\">devilish-witch-2</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact mnist-raw:latest, 98.19MB. 3 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "Done. 0:0:0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb04b107fd9b4afd8b7f7e2bf7bfa048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='4.721 MB of 4.721 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">devilish-witch-2</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/3nn1toac\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/3nn1toac</a><br/>Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221031_094432-3nn1toac/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(dataset, normalize=True, expand_dims=True):\n",
    "    x, y = dataset.tensors\n",
    "    if normalize:\n",
    "        # Scale images to the [0, 1] range\n",
    "        x = x.type(torch.float32) / 255\n",
    "    if expand_dims:\n",
    "        # Make sure images have shape (1, 28, 28)\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "    return TensorDataset(x, y)\n",
    "\n",
    "def preprocess_and_log(steps):\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"preprocess-data\") as run:\n",
    "        processed_data = wandb.Artifact(\n",
    "            \"mnist-preprocess\", type=\"dataset\",\n",
    "            description=\"Preprocessed MNIST dataset\",\n",
    "            metadata=steps)\n",
    "\n",
    "        # declare which artifact we'll be using\n",
    "        raw_data_artifact = run.use_artifact('mnist-raw:latest')\n",
    "\n",
    "        # if need be, download the artifact\n",
    "        raw_dataset = raw_data_artifact.download()\n",
    "        \n",
    "        for split in [\"training\", \"validation\", \"test\"]:\n",
    "            raw_split = read(raw_dataset, split)\n",
    "            processed_dataset = preprocess(raw_split, **steps)\n",
    "\n",
    "            with processed_data.new_file(split + \".pt\", mode=\"wb\") as file:\n",
    "                x, y = processed_dataset.tensors\n",
    "                torch.save((x, y), file)\n",
    "\n",
    "        run.log_artifact(processed_data)\n",
    "\n",
    "\n",
    "def read(data_dir, split):\n",
    "    filename = split + \".pt\"\n",
    "    x, y = torch.load(os.path.join(data_dir, filename))\n",
    "    return TensorDataset(x, y)\n",
    "\n",
    "steps = {\"normalize\": True, \"expand_dims\": True}\n",
    "\n",
    "preprocess_and_log(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221031_094441-17168xvu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/17168xvu\" target=\"_blank\">evil-phantasm-3</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">evil-phantasm-3</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/17168xvu\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/17168xvu</a><br/>Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221031_094441-17168xvu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from math import floor\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "      def __init__(self, hidden_layer_sizes=[32, 64],\n",
    "            kernel_sizes=[3],\n",
    "            activation=\"ReLU\",\n",
    "            pool_sizes=[2],\n",
    "            dropout=0.5,\n",
    "            num_classes=num_classes,\n",
    "            input_shape=input_shape):\n",
    "            super(ConvNet, self).__init__()\n",
    "\n",
    "            self.layer1 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels=input_shape[0], out_channels=hidden_layer_sizes[0], kernel_size=kernel_sizes[0]),\n",
    "                  getattr(nn, activation)(),\n",
    "                  nn.MaxPool2d(kernel_size=pool_sizes[0])\n",
    "            )\n",
    "            self.layer2 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels=hidden_layer_sizes[0], out_channels=hidden_layer_sizes[-1], kernel_size=kernel_sizes[-1]),\n",
    "                  getattr(nn, activation)(),\n",
    "                  nn.MaxPool2d(kernel_size=pool_sizes[-1])\n",
    "            )\n",
    "            self.layer3 = nn.Sequential(\n",
    "                  nn.Flatten(),\n",
    "                  nn.Dropout(dropout)\n",
    "            )\n",
    "\n",
    "            fc_input_dims = floor((input_shape[1] - kernel_sizes[0] + 1) / pool_sizes[0]) # layer 1 output size\n",
    "            fc_input_dims = floor((fc_input_dims - kernel_sizes[-1] + 1) / pool_sizes[-1]) # layer 2 output size\n",
    "            fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-1] # layer 3 output size\n",
    "\n",
    "            self.fc = nn.Linear(fc_input_dims, num_classes)\n",
    "\n",
    "      def forward(self, x):\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "def build_model_and_log(config):\n",
    "      with wandb.init(project=PROJECT_NAME, job_type=\"initialize\", config=config) as run:\n",
    "            config = wandb.config\n",
    "            model = ConvNet(**config)\n",
    "            model_artifact = wandb.Artifact(\n",
    "                  \"convnet\", type=\"model\",\n",
    "                  description=\"Simple AlexNet style CNN\",\n",
    "                  metadata=dict(config))\n",
    "\n",
    "            with model_artifact.new_file(\"initialized_model.pth\", mode=\"wb\") as file:\n",
    "                  torch.save(model.state_dict(), file)\n",
    "\n",
    "            run.log_artifact(model_artifact)\n",
    "\n",
    "model_config = {\"hidden_layer_sizes\": [32, 64],\n",
    "                  \"kernel_sizes\": [3],\n",
    "                  \"activation\": \"ReLU\",\n",
    "                  \"pool_sizes\": [2],\n",
    "                  \"dropout\": 0.5,\n",
    "                  \"num_classes\": 10}\n",
    "build_model_and_log(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Log an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221031_094448-25lx88se</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/25lx88se\" target=\"_blank\">crawly-ritual-4</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/750 (0%)]\tLoss: 2.330907\n",
      "Loss after 00128 examples: 2.331\n",
      "Loss/accuracy after 00750 examples: 2.149/59.600\n",
      "Train Epoch: 1 [0/750 (0%)]\tLoss: 2.078378\n",
      "Loss after 00134 examples: 2.078\n",
      "Loss/accuracy after 00756 examples: 1.804/65.600\n",
      "Train Epoch: 2 [0/750 (0%)]\tLoss: 1.677050\n",
      "Loss after 00140 examples: 1.677\n",
      "Loss/accuracy after 00762 examples: 1.285/74.800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9384542e9ab9475095db706fdf4a2a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.272 MB of 0.272 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▅█</td></tr><tr><td>train/loss</td><td>▁</td></tr><tr><td>validation/accuracy</td><td>▁▄█</td></tr><tr><td>validation/loss</td><td>█▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train/loss</td><td>2.33091</td></tr><tr><td>validation/accuracy</td><td>74.8</td></tr><tr><td>validation/loss</td><td>1.28515</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">crawly-ritual-4</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/25lx88se\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/25lx88se</a><br/>Synced 7 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221031_094448-25lx88se/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221031_094455-3o8zo3r4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/3o8zo3r4\" target=\"_blank\">evil-spirit-5</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>77.0</td></tr><tr><td>loss</td><td>1.3132</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">evil-spirit-5</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/3o8zo3r4\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/3o8zo3r4</a><br/>Synced 7 W&B file(s), 32 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221031_094455-3o8zo3r4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, train_loader, valid_loader, config):\n",
    "    optimizer = getattr(torch.optim, config.optimizer)(model.parameters())\n",
    "    model.train()\n",
    "    for epoch in range(config.epochs):\n",
    "        train_epoch(model, train_loader, valid_loader, config.batch_log_interval, optimizer, epoch)\n",
    "\n",
    "def train_epoch(model, train_loader, valid_loader, batch_log_interval, optimizer, epoch):\n",
    "    example_ct = epoch * len(train_loader)\n",
    "    cumu_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        cumu_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        example_ct += len(data)\n",
    "        if batch_idx % batch_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0%})]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                batch_idx / len(train_loader), loss.item()))\n",
    "            train_log(loss, example_ct, epoch)\n",
    "\n",
    "    if not valid_loader is None:\n",
    "        # evaluate the model on the validation set at each epoch\n",
    "        loss, accuracy = test(model, valid_loader)\n",
    "        test_log(loss, accuracy, example_ct, epoch)\n",
    "\n",
    "    return cumu_loss / len(train_loader)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum')  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"train/loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    \n",
    "def test_log(loss, accuracy, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    accuracy = float(accuracy)\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"validation/loss\": loss, \"validation/accuracy\": accuracy}, step=example_ct)\n",
    "    print(f\"Loss/accuracy after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}/{accuracy:.3f}\")\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    loss, accuracy = test(model, test_loader)\n",
    "    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)\n",
    "    return loss, accuracy, highest_losses, hardest_examples, true_labels, predictions\n",
    "\n",
    "def get_hardest_k_examples(model, testing_set, k=32):\n",
    "    model.eval()\n",
    "    loader = DataLoader(testing_set, 1, shuffle=False)\n",
    "    # get the losses and predictions for each item in the dataset\n",
    "    losses = None\n",
    "    predictions = None\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            if losses is None:\n",
    "                losses = loss.view((1, 1))\n",
    "                predictions = pred\n",
    "            else:\n",
    "                losses = torch.cat((losses, loss.view((1, 1))), 0)\n",
    "                predictions = torch.cat((predictions, pred), 0)\n",
    "\n",
    "    argsort_loss = torch.argsort(losses, dim=0)\n",
    "    highest_k_losses = losses[argsort_loss[-k:]]\n",
    "    hardest_k_examples = testing_set[argsort_loss[-k:]][0]\n",
    "    true_labels = testing_set[argsort_loss[-k:]][1]\n",
    "    predicted_labels = predictions[argsort_loss[-k:]]\n",
    "    return highest_k_losses, hardest_k_examples, true_labels, predicted_labels\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_and_log(config):\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"train\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        data = run.use_artifact('mnist-preprocess:latest')\n",
    "        data_dir = data.download()\n",
    "\n",
    "        training_dataset = read(data_dir, \"training\")\n",
    "        validation_dataset = read(data_dir, \"validation\")\n",
    "        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)\n",
    "        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)\n",
    "        \n",
    "        model_artifact = run.use_artifact(\"convnet:latest\")\n",
    "        model_dir = model_artifact.download()\n",
    "        model_path = os.path.join(model_dir, \"initialized_model.pth\")\n",
    "        model_config = model_artifact.metadata\n",
    "        config.update(model_config)\n",
    "\n",
    "        model = ConvNet(**model_config)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model = model.to(device)\n",
    "\n",
    "        train(model, train_loader, validation_loader, config)\n",
    "        model_artifact = wandb.Artifact(\n",
    "            \"trained-model\", type=\"model\",\n",
    "            description=\"Trained NN model\",\n",
    "            metadata=dict(model_config))\n",
    "\n",
    "        with model_artifact.new_file(\"trained_model.pth\", mode=\"wb\") as file:\n",
    "            torch.save(model.state_dict(), file)\n",
    "\n",
    "        run.log_artifact(model_artifact)\n",
    "\n",
    "    return model\n",
    "\n",
    "    \n",
    "def evaluate_and_log(config=None):\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"report\", config=config) as run:\n",
    "        data = run.use_artifact('mnist-preprocess:latest')\n",
    "        data_dir = data.download()\n",
    "        testing_set = read(data_dir, \"test\")\n",
    "        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=128, shuffle=False)\n",
    "\n",
    "        model_artifact = run.use_artifact(\"trained-model:latest\")\n",
    "        model_dir = model_artifact.download()\n",
    "        model_path = os.path.join(model_dir, \"trained_model.pth\")\n",
    "        model_config = model_artifact.metadata\n",
    "\n",
    "        model = ConvNet(**model_config)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.to(device)\n",
    "\n",
    "        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)\n",
    "        run.summary.update({\"loss\": loss, \"accuracy\": accuracy})\n",
    "\n",
    "        wandb.log({\"high-loss-examples\":\n",
    "            [wandb.Image(hard_example, caption=str(int(pred)) + \",\" +  str(int(label)))\n",
    "                for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})\n",
    "\n",
    "train_config = {\"batch_size\": 128,\n",
    "                \"epochs\": 3,\n",
    "                \"batch_log_interval\": 25,\n",
    "                \"optimizer\": \"Adam\"}\n",
    "\n",
    "model = train_and_log(train_config)\n",
    "evaluate_and_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Define Sweep config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'random',\n",
       " 'metric': {'name': 'loss', 'goal': 'minimize'},\n",
       " 'parameters': {'epochs': {'value': 1},\n",
       "  'optimizer': {'values': ['adam', 'sgd']},\n",
       "  'hidden_layer_1_size': {'values': [16, 32]},\n",
       "  'hidden_layer_2_size': {'values': [32, 64]},\n",
       "  'dropout': {'values': [0.4, 0.5]},\n",
       "  'learning_rate': {'distribution': 'uniform', 'min': 0, 'max': 0.1},\n",
       "  'batch_size': {'distribution': 'q_log_uniform_values',\n",
       "   'q': 8,\n",
       "   'min': 32,\n",
       "   'max': 256}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    # For bayesian Sweeps only: need to know what to minimize\n",
    "    'metric': {\n",
    "        'name': 'loss',\n",
    "        'goal': 'minimize',\n",
    "    },\n",
    "    'parameters': {\n",
    "        # epochs var doesnt vary, but we still want it here\n",
    "        'epochs': {\n",
    "            'value': 1,\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'sgd'],\n",
    "        },\n",
    "        'hidden_layer_1_size': {\n",
    "            'values': [16, 32],\n",
    "        },\n",
    "        'hidden_layer_2_size': {\n",
    "            'values': [32, 64],\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.4, 0.5],\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0,\n",
    "            'max': 0.1\n",
    "        },\n",
    "        'batch_size': {\n",
    "            # integers between 32 and 256\n",
    "            # with evenly-distributed logarithms \n",
    "            'distribution': 'q_log_uniform_values',\n",
    "            'q': 8,\n",
    "            'min': 32,\n",
    "            'max': 256,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Run the Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 09s1x4pa\n",
      "Sweep URL: http://localhost:8080/demo/soict-2022/sweeps/09s1x4pa\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_log_interval = 25\n",
    "\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        loader = build_dataset(run, config)\n",
    "        model = build_model(run, config)\n",
    "        optimizer = build_optimizer(model, config)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_loss = train_epoch(model, loader, None, batch_log_interval, optimizer, epoch)\n",
    "            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})\n",
    "\n",
    "def build_dataset(run, config):\n",
    "    batch_size = config.batch_size\n",
    "    data = run.use_artifact('mnist-preprocess:latest')\n",
    "    data_dir = data.download()\n",
    "    training_dataset = read(data_dir, \"training\")\n",
    "    sub_dataset = torch.utils.data.Subset(\n",
    "        training_dataset, indices=range(0, len(training_dataset), 5))\n",
    "    train_loader = DataLoader(sub_dataset, batch_size=batch_size)\n",
    "    return train_loader\n",
    "\n",
    "def build_model(run, config):\n",
    "    model_config = {\n",
    "        'hidden_layer_sizes': [\n",
    "            config.hidden_layer_1_size,\n",
    "            config.hidden_layer_2_size,\n",
    "        ],\n",
    "        'dropout': config.dropout,\n",
    "    }\n",
    "    model = ConvNet(**model_config)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "        \n",
    "def build_optimizer(model, config):\n",
    "    optimizer = config.optimizer\n",
    "    learning_rate = config.learning_rate\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5r5l7ys2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.015102956640100463\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221031_094519-5r5l7ys2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/5r5l7ys2\" target=\"_blank\">glowing-sweep-1</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022/sweeps/09s1x4pa\" target=\"_blank\">http://localhost:8080/demo/soict-2022/sweeps/09s1x4pa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/150 (0%)]\tLoss: 2.306972\n",
      "Loss after 00144 examples: 2.307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efa29faa8b44757abacaa4b653b4374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.137 MB of 0.137 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>2.92674</td></tr><tr><td>train/loss</td><td>2.30697</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">glowing-sweep-1</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/5r5l7ys2\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/5r5l7ys2</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221031_094519-5r5l7ys2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8s7gjy7q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.007609440830379311\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221031_094530-8s7gjy7q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/8s7gjy7q\" target=\"_blank\">light-sweep-2</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022/sweeps/09s1x4pa\" target=\"_blank\">http://localhost:8080/demo/soict-2022/sweeps/09s1x4pa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/150 (0%)]\tLoss: 2.295985\n",
      "Loss after 00064 examples: 2.296\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01da37b0dadf40c386b033079a95106f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.137 MB of 0.137 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>2.30869</td></tr><tr><td>train/loss</td><td>2.29599</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">light-sweep-2</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/8s7gjy7q\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/8s7gjy7q</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221031_094530-8s7gjy7q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ted3giw8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.051657374679097805\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221031_094539-ted3giw8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/ted3giw8\" target=\"_blank\">usual-sweep-3</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022/sweeps/09s1x4pa\" target=\"_blank\">http://localhost:8080/demo/soict-2022/sweeps/09s1x4pa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/150 (0%)]\tLoss: 2.297935\n",
      "Loss after 00048 examples: 2.298\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a305ee184474870a5e3953e1c17370f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.137 MB of 0.137 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>4.37686</td></tr><tr><td>train/loss</td><td>2.29794</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">usual-sweep-3</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/ted3giw8\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/ted3giw8</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221031_094539-ted3giw8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aumhgvcv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.06045335483128189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221031_094550-aumhgvcv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/aumhgvcv\" target=\"_blank\">valiant-sweep-4</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022/sweeps/09s1x4pa\" target=\"_blank\">http://localhost:8080/demo/soict-2022/sweeps/09s1x4pa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/150 (0%)]\tLoss: 2.299828\n",
      "Loss after 00040 examples: 2.300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b3890ee5944a6bb8d38d8779e55e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.123 MB of 0.123 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>2.28561</td></tr><tr><td>train/loss</td><td>2.29983</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">valiant-sweep-4</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/aumhgvcv\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/aumhgvcv</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221031_094550-aumhgvcv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i7qbqsmd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 152\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0011304934838524017\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221031_094600-i7qbqsmd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/i7qbqsmd\" target=\"_blank\">grateful-sweep-5</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022/sweeps/09s1x4pa\" target=\"_blank\">http://localhost:8080/demo/soict-2022/sweeps/09s1x4pa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/150 (0%)]\tLoss: 2.306427\n",
      "Loss after 00150 examples: 2.306\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4b507794434672aefd8993c5654a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.123 MB of 0.123 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>2.30643</td></tr><tr><td>train/loss</td><td>2.30643</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">grateful-sweep-5</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/i7qbqsmd\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/i7qbqsmd</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221031_094600-i7qbqsmd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train_sweep, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Stopping sweep demo/soict-2022/09s1x4pa.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Done.\n"
     ]
    }
   ],
   "source": [
    "# For self-hosted Wandb server\n",
    "!wandb sweep --stop \"demo/$PROJECT_NAME/$sweep_id\"\n",
    "\n",
    "# For Wandb cloud server\n",
    "# Stop sweep at https://wandb.ai/<wandb-user>/soict-2022/sweeps/<sweep-id>/controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('soict')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "291bacc89d47c570d681c8cb8133df5aeea2ebc003da29abf8c072691acbbc14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
