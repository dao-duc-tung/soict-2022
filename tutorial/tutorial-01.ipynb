{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "In this notebook, I'll show you how to use Wandb to perform the following tasks in a typical ML workflow:\n",
    "\n",
    "1. Data versioning\n",
    "1. Experiment tracking\n",
    "1. Hyperparameter tuning\n",
    "\n",
    "In order to run this notebook, please follow the instruction in the `README.md` file to setup your working environment with Wandb installed and your Wandb account logged in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"soict-2022\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data versioning\n",
    "\n",
    "> Those that fail to learn from history are doomed to repeat it. - Winston Churchill\n",
    "\n",
    "In Wandb, an `Artifact` is the input or output of a process. A `Run` is a task that we want to perform.\n",
    "\n",
    "In ML, the most important artifacts are _datasets_ and _models_. They should be organized so that you can learn from them.\n",
    "\n",
    "In Wandb, we can log `Artifact` as ouputs of Wandb `Run`s or use `Artifact` as input to `Run`s, as in this diagram, where a training run takes in a dataset and produces a model.\n",
    "\n",
    "![wandb-artifact-run](assets/wandb-artifact-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Log a dataset\n",
    "\n",
    "This example uses the MNIST database. The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "\n",
    "![mnist-examples](assets/mnist-exmaples.png)\n",
    "\n",
    "We start with the `Dataset`s:\n",
    "\n",
    "- A training set and a validation set, for model training\n",
    "- A test set, for model evaluation\n",
    "\n",
    "The cell below defines these three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data parameters\n",
    "num_classes = 10\n",
    "input_shape = (1, 28, 28)\n",
    "N_TRAIN_VALID = 1000\n",
    "N_TEST = 200\n",
    "\n",
    "# drop slow mirror from list of MNIST mirrors\n",
    "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
    "                                        if not mirror.startswith(\"http://yann.lecun.com\")]\n",
    "\n",
    "def load(n_train_valid=N_TRAIN_VALID, n_test=N_TEST):\n",
    "    # split between train and test sets\n",
    "    train = torchvision.datasets.MNIST(\"./\", train=True, download=True)\n",
    "    test = torchvision.datasets.MNIST(\"./\", train=False, download=True)\n",
    "    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)\n",
    "    x_train = x_train[:n_train_valid]\n",
    "    y_train = y_train[:n_train_valid]\n",
    "    x_test = x_test[:n_test]\n",
    "    y_test = y_test[:n_test]\n",
    "\n",
    "    # split off a validation set for hyperparameter tuning\n",
    "    train_size = int(n_train_valid * 0.75)\n",
    "    x_train, x_val = x_train[:train_size], x_train[train_size:]\n",
    "    y_train, y_val = y_train[:train_size], y_train[train_size:]\n",
    "\n",
    "    training_set = TensorDataset(x_train, y_train)\n",
    "    validation_set = TensorDataset(x_val, y_val)\n",
    "    test_set = TensorDataset(x_test, y_test)\n",
    "    datasets = [training_set, validation_set, test_set]\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to log these datasets as Artifacts, we just need to:\n",
    "\n",
    "1. Create a Run with `wandb.init`\n",
    "1. Create an Artifact for the dataset\n",
    "1. Save and log the associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_log():\n",
    "    # start a run, with a type to label it and a project name\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"load-data\") as run:\n",
    "        datasets = load()  # separate code for loading the datasets\n",
    "        names = [\"training\", \"validation\", \"test\"]\n",
    "\n",
    "        # create our Artifact\n",
    "        raw_data = wandb.Artifact(\n",
    "            \"mnist-raw\", type=\"dataset\",\n",
    "            description=\"Raw MNIST dataset, split into train/val/test\",\n",
    "            metadata={\"source\": \"torchvision.datasets.MNIST\",\n",
    "                        \"sizes\": [len(dataset) for dataset in datasets]})\n",
    "\n",
    "        for name, data in zip(names, datasets):\n",
    "            # Store a new file in the artifact, and write data\n",
    "            with raw_data.new_file(name + \".pt\", mode=\"wb\") as file:\n",
    "                x, y = data.tensors\n",
    "                torch.save((x, y), file)\n",
    "\n",
    "        # Save the artifact to Wandb\n",
    "        run.log_artifact(raw_data)\n",
    "\n",
    "load_and_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wandb.init\n",
    "\n",
    "`Run` defines which Wandb project we want to run.\n",
    "\n",
    "Artifacts that are logged will be kept inside a single Wandb project. This keeps things simple, but Artifacts are portable across projects!\n",
    "\n",
    "To keep track of different types of jobs, it's useful to provide a `job_type` when making Runs. This keeps the graph of your Artifacts nice and tidy.\n",
    "\n",
    "**Note**: the `job_type` should be descriptive and correspond to a single step of your pipeline. Here, we separate out loading data from preprocessing data.\n",
    "\n",
    "<br>\n",
    "\n",
    "### wandb.Artifact\n",
    "\n",
    "To log an Artifact, we make an Artifact object with a name.\n",
    "\n",
    "**Note**: the name should be descriptive, hyphen-separated, and correspond to variable names in the code.\n",
    "\n",
    "An Artifact also has a type. Just like `job_type` for Runs, this is used for organizing the graph of Runs and Artifacts.\n",
    "\n",
    "You can attach a description and some metadata as a dictionary. The metadata needs to be serializable to JSON.\n",
    "\n",
    "<br>\n",
    "\n",
    "### artifact.new_file and run.log_artifact\n",
    "\n",
    "Once we've made an Artifact object, we need to add files to it.\n",
    "\n",
    "Artifacts are structured like directories, with files and sub-directories.\n",
    "\n",
    "We use the `new_file` method to simultaneously write the file and attach it to the Artifact. We also use the `add_file` method, which separates those two steps.\n",
    "\n",
    "Once we've added all of our files, we call `log_artifact` to upload artifacts to wandb.ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Preprocess a logged dataset artifact\n",
    "\n",
    "Artifacts are designed to be used, not just stored.\n",
    "\n",
    "The cell below defines a pipeline step that takes in a raw dataset and uses it to produce a preprocessed dataset: normalized and shaped correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset, normalize=True, expand_dims=True):\n",
    "    x, y = dataset.tensors\n",
    "    if normalize:\n",
    "        # Scale images to the [0, 1] range\n",
    "        x = x.type(torch.float32) / 255\n",
    "    if expand_dims:\n",
    "        # Make sure images have shape (1, 28, 28)\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "    return TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below instruments the preprocess step with `wandb.Artifact` logging. It uses Artifacts as both the inputs and the outputs of Runs!\n",
    "\n",
    "We use `job_type=\"preprocess-data\"` to state that this is a different job type from the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_log(steps):\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"preprocess-data\") as run:\n",
    "        # declare which artifact we'll be using and download it\n",
    "        raw_data_artifact = run.use_artifact('mnist-raw:latest')\n",
    "        raw_dataset_dir = raw_data_artifact.download()\n",
    "        \n",
    "        processed_data = wandb.Artifact(\n",
    "            \"mnist-preprocess\", type=\"dataset\",\n",
    "            description=\"Preprocessed MNIST dataset\",\n",
    "            metadata=steps)\n",
    "        \n",
    "        for ds_name in [\"training\", \"validation\", \"test\"]:\n",
    "            raw_split = read(raw_dataset_dir, ds_name)\n",
    "            processed_dataset = preprocess(raw_split, **steps)\n",
    "\n",
    "            with processed_data.new_file(ds_name + \".pt\", mode=\"wb\") as file:\n",
    "                x, y = processed_dataset.tensors\n",
    "                torch.save((x, y), file)\n",
    "\n",
    "        run.log_artifact(processed_data)\n",
    "\n",
    "\n",
    "def read(data_dir, ds_name):\n",
    "    filename = ds_name + \".pt\"\n",
    "    x, y = torch.load(os.path.join(data_dir, filename))\n",
    "    return TensorDataset(x, y)\n",
    "\n",
    "steps = {\"normalize\": True, \"expand_dims\": True}\n",
    "\n",
    "preprocess_and_log(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run.use_artifact\n",
    "\n",
    "To use an Artifact, we need to know its name and its version.\n",
    "\n",
    "By default, the last uploaded version is tagged as `latest`. Versions are separated from names with `:`, so the Artifact we want is `mnist-raw:latest`.\n",
    "\n",
    "<br>\n",
    "\n",
    "### artifact.download\n",
    "\n",
    "Before we actually download anything, we check to see if the right version is available locally by using *hashing*.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: the steps of the preprocessing are saved with the `preprocessed_data` as metadata.\n",
    "\n",
    "If you're trying to make your experiments reproducible, capturing lots of metadata is a good idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Initialize a model\n",
    "\n",
    "This example show us how Artifacts can improve your ML workflow.\n",
    "\n",
    "This cell below builds a simple Convolutional Neurnet Net model in PyTorch.\n",
    "\n",
    "We'll start by just initializing the model, not training it. That way, we can repeat the training while keeping everything else constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "      def __init__(self, hidden_layer_sizes=[32, 64],\n",
    "            kernel_sizes=[3],\n",
    "            activation=\"ReLU\",\n",
    "            pool_sizes=[2],\n",
    "            dropout=0.5,\n",
    "            num_classes=num_classes,\n",
    "            input_shape=input_shape):\n",
    "            super(ConvNet, self).__init__()\n",
    "\n",
    "            self.layer1 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels=input_shape[0], out_channels=hidden_layer_sizes[0], kernel_size=kernel_sizes[0]),\n",
    "                  getattr(nn, activation)(),\n",
    "                  nn.MaxPool2d(kernel_size=pool_sizes[0])\n",
    "            )\n",
    "            self.layer2 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels=hidden_layer_sizes[0], out_channels=hidden_layer_sizes[-1], kernel_size=kernel_sizes[-1]),\n",
    "                  getattr(nn, activation)(),\n",
    "                  nn.MaxPool2d(kernel_size=pool_sizes[-1])\n",
    "            )\n",
    "            self.layer3 = nn.Sequential(\n",
    "                  nn.Flatten(),\n",
    "                  nn.Dropout(dropout)\n",
    "            )\n",
    "\n",
    "            fc_input_dims = floor((input_shape[1] - kernel_sizes[0] + 1) / pool_sizes[0]) # layer 1 output size\n",
    "            fc_input_dims = floor((fc_input_dims - kernel_sizes[-1] + 1) / pool_sizes[-1]) # layer 2 output size\n",
    "            fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-1] # layer 3 output size\n",
    "\n",
    "            self.fc = nn.Linear(fc_input_dims, num_classes)\n",
    "\n",
    "      def forward(self, x):\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.fc(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we're using Wandb to track the run, and so using the `wandb.config` object to store all of the hyperparameters.\n",
    "\n",
    "The dictionary version of that config object is a really useful piece of metadata, so make sure to include it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_and_log(config):\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"initialize\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        model = ConvNet(**config)\n",
    "        model_artifact = wandb.Artifact(\n",
    "            \"convnet\", type=\"model\",\n",
    "            description=\"Simple AlexNet style CNN\",\n",
    "            metadata=dict(config))\n",
    "\n",
    "        with model_artifact.new_file(\"initialized_model.pth\", mode=\"wb\") as file:\n",
    "            torch.save(model.state_dict(), file)\n",
    "\n",
    "        run.log_artifact(model_artifact)\n",
    "\n",
    "model_config = {\"hidden_layer_sizes\": [32, 64],\n",
    "                \"kernel_sizes\": [3],\n",
    "                \"activation\": \"ReLU\",\n",
    "                \"pool_sizes\": [2],\n",
    "                \"dropout\": 0.5,\n",
    "                \"num_classes\": 10}\n",
    "\n",
    "build_model_and_log(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: we changed the type of the Artifact to `model`, rather than `dataset`. Runs that produce models will be separated from those that produce datasets in the graph view on the Artifacts page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Log an experiment\n",
    "\n",
    "Just like we could call `use_artifact` on a dataset, we can call it on our `initialized_model` to use it in another `Run`.\n",
    "\n",
    "This time, let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, train_loader, valid_loader, config):\n",
    "    optimizer = getattr(torch.optim, config.optimizer)(model.parameters())\n",
    "    model.train()\n",
    "    for epoch in range(config.epochs):\n",
    "        train_epoch(model, train_loader, valid_loader, config.batch_log_interval, optimizer, epoch)\n",
    "\n",
    "def train_epoch(model, train_loader, valid_loader, batch_log_interval, optimizer, epoch):\n",
    "    example_ct = epoch * len(train_loader)\n",
    "    cumu_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        cumu_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        example_ct += len(data)\n",
    "        if batch_idx % batch_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0%})]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                batch_idx / len(train_loader), loss.item()))\n",
    "            train_log(loss, example_ct, epoch)\n",
    "\n",
    "    if not valid_loader is None:\n",
    "        # evaluate the model on the validation set at each epoch\n",
    "        loss, accuracy = test(model, valid_loader)\n",
    "        test_log(loss, accuracy, example_ct, epoch)\n",
    "\n",
    "    return cumu_loss / len(train_loader)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum')  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"train/loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    \n",
    "def test_log(loss, accuracy, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    accuracy = float(accuracy)\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"validation/loss\": loss, \"validation/accuracy\": accuracy}, step=example_ct)\n",
    "    print(f\"Loss/accuracy after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}/{accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run two separate Artifact-producing Runs this time.\n",
    "\n",
    "Once the first finishes training the model, the second will consume the trained model Artifact by evaluating its performance on the `test_dataset`.\n",
    "\n",
    "We also select the most confused 32 examples -- on which the `categorical_crossentropy` is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    loss, accuracy = test(model, test_loader)\n",
    "    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)\n",
    "    return loss, accuracy, highest_losses, hardest_examples, true_labels, predictions\n",
    "\n",
    "def get_hardest_k_examples(model, testing_set, k=32):\n",
    "    model.eval()\n",
    "    loader = DataLoader(testing_set, 1, shuffle=False)\n",
    "    # get the losses and predictions for each item in the dataset\n",
    "    losses = None\n",
    "    predictions = None\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            if losses is None:\n",
    "                losses = loss.view((1, 1))\n",
    "                predictions = pred\n",
    "            else:\n",
    "                losses = torch.cat((losses, loss.view((1, 1))), 0)\n",
    "                predictions = torch.cat((predictions, pred), 0)\n",
    "\n",
    "    argsort_loss = torch.argsort(losses, dim=0)\n",
    "    highest_k_losses = losses[argsort_loss[-k:]]\n",
    "    hardest_k_examples = testing_set[argsort_loss[-k:]][0]\n",
    "    true_labels = testing_set[argsort_loss[-k:]][1]\n",
    "    predicted_labels = predictions[argsort_loss[-k:]]\n",
    "    return highest_k_losses, hardest_k_examples, true_labels, predicted_labels\n",
    "\n",
    "def train_and_log(config):\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"train\", config=config) as run:\n",
    "        config = wandb.config\n",
    "        data = run.use_artifact('mnist-preprocess:latest')\n",
    "        data_dir = data.download()\n",
    "\n",
    "        training_dataset = read(data_dir, \"training\")\n",
    "        validation_dataset = read(data_dir, \"validation\")\n",
    "        train_loader = DataLoader(training_dataset, batch_size=config.batch_size)\n",
    "        validation_loader = DataLoader(validation_dataset, batch_size=config.batch_size)\n",
    "        \n",
    "        model_artifact = run.use_artifact(\"convnet:latest\")\n",
    "        model_dir = model_artifact.download()\n",
    "        model_path = os.path.join(model_dir, \"initialized_model.pth\")\n",
    "        model_config = model_artifact.metadata\n",
    "        config.update(model_config)\n",
    "\n",
    "        model = ConvNet(**model_config)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model = model.to(device)\n",
    "\n",
    "        train(model, train_loader, validation_loader, config)\n",
    "        \n",
    "        model_artifact = wandb.Artifact(\n",
    "            \"trained-model\", type=\"model\",\n",
    "            description=\"Trained NN model\",\n",
    "            metadata=dict(model_config))\n",
    "\n",
    "        with model_artifact.new_file(\"trained_model.pth\", mode=\"wb\") as file:\n",
    "            torch.save(model.state_dict(), file)\n",
    "\n",
    "        run.log_artifact(model_artifact)\n",
    "\n",
    "    return model\n",
    "    \n",
    "def evaluate_and_log(config=None):\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"report\", config=config) as run:\n",
    "        data = run.use_artifact('mnist-preprocess:latest')\n",
    "        data_dir = data.download()\n",
    "        testing_set = read(data_dir, \"test\")\n",
    "        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=128, shuffle=False)\n",
    "\n",
    "        model_artifact = run.use_artifact(\"trained-model:latest\")\n",
    "        model_dir = model_artifact.download()\n",
    "        model_path = os.path.join(model_dir, \"trained_model.pth\")\n",
    "        model_config = model_artifact.metadata\n",
    "\n",
    "        model = ConvNet(**model_config)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.to(device)\n",
    "\n",
    "        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)\n",
    "        run.summary.update({\"loss\": loss, \"accuracy\": accuracy})\n",
    "\n",
    "        wandb.log({\"high-loss-examples\":\n",
    "            [wandb.Image(hard_example, caption=str(int(pred)) + \",\" +  str(int(label)))\n",
    "                for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})\n",
    "\n",
    "train_config = {\"batch_size\": 128,\n",
    "                \"epochs\": 3,\n",
    "                \"batch_log_interval\": 25,\n",
    "                \"optimizer\": \"Adam\"}\n",
    "\n",
    "model = train_and_log(train_config)\n",
    "evaluate_and_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning\n",
    "\n",
    "In Wandb, *Hyperparameter Sweeps* provide an organized and efficient way to search through high dimensional hyperparameter spaces to find the most performant model.\n",
    "\n",
    "They enable this by automatically searching through combinations of hyperparameter values (e.g. learning rate, batch size, number of hidden layers, optimizer type) to find the most optimal values.\n",
    "\n",
    "![wandb-sweep-overview](assets/wandb-sweep-overview.png)\n",
    "\n",
    "To run a hyperparameter sweep with Wandb, there are 3 simple steps:\n",
    "\n",
    "1.  Define the sweep configuration\n",
    "\n",
    "    We do this by creating a dictionary that specifies the search strategy, optimization metric, and parameters to search through.\n",
    "\n",
    "1.  Initialize the sweep\n",
    "    \n",
    "    We initialize the sweep and pass in the dictionary of sweep configurations\n",
    "\n",
    "    ```bash\n",
    "    sweep_id = wandb.sweep(sweep_config)\n",
    "    ```\n",
    "\n",
    "1.  Run the sweep agent\n",
    "    \n",
    "    We call `wandb.agent()` and pass the `sweep_id` to run, along with a function that defines your training steps:\n",
    "\n",
    "    ```bash\n",
    "    wandb.agent(sweep_id, function=train)\n",
    "    ```\n",
    "\n",
    "In this section, we'll see how you can run sophisticated hyperparameter sweeps using Wandb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Define Sweep config\n",
    "\n",
    "A Sweep combines a strategy for trying out a bunch of hyperparameter values with the code that evalutes them.\n",
    "\n",
    "#### Pick a method\n",
    "The first thing we need to define is the method for choosing new parameter values. It can be:\n",
    "\n",
    "- `grid` Search – Iterate over every combination of hyperparameter values. Very effective, but can be computationally costly.\n",
    "- `random` Search – Select each new combination at random according to provided distributions. Surprisingly effective!\n",
    "- `bayesian` Search – Create a probabilistic model of metric score as a function of the hyperparameters, and choose parameters with high probability of improving the metric. Works well for small numbers of continuous parameters but scales poorly.\n",
    "\n",
    "We select `random` Search for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've picked a method to try out new values of the hyperparameters, you need to define what those parameters are.\n",
    "\n",
    "This step is straightforward: just give the parameter a name and specify a list of legal values of the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    # epochs var doesn't vary, but we still want it here\n",
    "    'epochs': {\n",
    "        'value': 1,\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd'],\n",
    "    },\n",
    "    'hidden_layer_1_size': {\n",
    "        'values': [16, 32],\n",
    "    },\n",
    "    'hidden_layer_2_size': {\n",
    "        'values': [32, 64],\n",
    "    },\n",
    "    'dropout': {\n",
    "        'values': [0.4, 0.5],\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0,\n",
    "        'max': 0.1\n",
    "    },\n",
    "    'batch_size': {\n",
    "        # integers between 32 and 256\n",
    "        # with evenly-distributed logarithms \n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q': 8,\n",
    "        'min': 32,\n",
    "        'max': 256,\n",
    "    }\n",
    "}\n",
    "sweep_config['parameters'] = parameters\n",
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wandb also offers the option to `early_terminate` your runs with the `HyperBand` scheduling algorithm. See more [here](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#early_terminate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Run the Sweep\n",
    "\n",
    "The Sweep Controller is in charge of our Sweep. The Sweep Controller instructs how to run each set of hyperparameters via Agents.\n",
    "\n",
    "In a typical Sweep, the Controller lives on Wandb's server, while the agents who complete runs live on your machine(s). This makes it easy to scale up Sweeps by just adding more machines to run agents!\n",
    "\n",
    "![sweeps-diagram](assets/sweeps-diagram.png)\n",
    "\n",
    "We can initialize a Sweep Controller by calling `wandb.sweep` with `sweep_config` and project name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can actually execute the sweep, we need to define the training procedure used in the Sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_log_interval = 25\n",
    "\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        loader = build_dataset(run, config)\n",
    "        model = build_model(run, config)\n",
    "        optimizer = build_optimizer(model, config)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_loss = train_epoch(model, loader, None, batch_log_interval, optimizer, epoch)\n",
    "            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines: `build_dataset`, `build_model`, and `build_optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(run, config):\n",
    "    batch_size = config.batch_size\n",
    "    data = run.use_artifact('mnist-preprocess:latest')\n",
    "    data_dir = data.download()\n",
    "    training_dataset = read(data_dir, \"training\")\n",
    "    sub_dataset = torch.utils.data.Subset(\n",
    "        training_dataset, indices=range(0, len(training_dataset), 5))\n",
    "    train_loader = DataLoader(sub_dataset, batch_size=batch_size)\n",
    "    return train_loader\n",
    "\n",
    "def build_model(run, config):\n",
    "    model_config = {\n",
    "        'hidden_layer_sizes': [\n",
    "            config.hidden_layer_1_size,\n",
    "            config.hidden_layer_2_size,\n",
    "        ],\n",
    "        'dropout': config.dropout,\n",
    "    }\n",
    "    model = ConvNet(**model_config)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "        \n",
    "def build_optimizer(model, config):\n",
    "    optimizer = config.optimizer\n",
    "    learning_rate = config.learning_rate\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will launch an agent that runs train 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train_sweep, count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Visualize Sweep Results\n",
    "\n",
    "Go to `https://wandb.ai/<wandb-user>/soict-2022/sweeps/<sweep-id>` to see the Sweep results.\n",
    "\n",
    "#### Parallel Coordinates Plot\n",
    "\n",
    "This plot maps hyperparameter values to model metrics.\n",
    "\n",
    "![hyperparam-plot](assets/hyperparam-plot.png)\n",
    "\n",
    "#### Hyperparameter Importance Plot\n",
    "\n",
    "The hyperparameter importance plot surfaces which hyperparameters were the best predictors of your metrics. Wandb reports feature importance (using a random forest model) and correlation (using a linear model).\n",
    "\n",
    "![hyperparam-importance](assets/hyperparam-importance.png)\n",
    "\n",
    "These visualizations can help you save both time and resources running expensive hyperparameter optimizations by refining the parameters (and value ranges), and thereby worthy of further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Stop the Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For self-hosted Wandb server\n",
    "!wandb sweep --stop \"demo/$PROJECT_NAME/$sweep_id\"\n",
    "\n",
    "# For Wandb cloud server\n",
    "# Stop sweep at https://wandb.ai/<wandb-user>/soict-2022/sweeps/<sweep-id>/controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('soict')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "291bacc89d47c570d681c8cb8133df5aeea2ebc003da29abf8c072691acbbc14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
