{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "In this notebook, I'll show you how to use Wandb to perform the following tasks in a typical ML workflow:\n",
    "\n",
    "1. Data versioning\n",
    "1. Experiment tracking\n",
    "1. Hyperparameter tuning\n",
    "\n",
    "In order to run this notebook, please follow the instruction in the `README.md` file to setup your working environment with Wandb installed and your Wandb account logged in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"soict-2022\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data versioning\n",
    "\n",
    "> Those that fail to learn from history are doomed to repeat it. - Winston Churchill\n",
    "\n",
    "In Wandb, an `Artifact` is the input or output of a process. A `Run` is a task that we want to perform.\n",
    "\n",
    "In ML, the most important artifacts are _datasets_ and _models_. They should be organized so that you can learn from them.\n",
    "\n",
    "In Wandb, we can log `Artifact` as ouputs of Wandb `Run`s or use `Artifact` as input to `Run`s, as in this diagram, where a training run takes in a dataset and produces a model.\n",
    "\n",
    "![wandb-artifact-run](assets/wandb-artifact-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses the MNIST database. The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "\n",
    "![mnist-examples](assets/mnist-exmaples.png)\n",
    "\n",
    "We start with the `Dataset`s:\n",
    "\n",
    "- A training set and a validation set, for model training\n",
    "- A test set, for model evaluation\n",
    "\n",
    "The cell below defines these three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data parameters\n",
    "num_classes = 10\n",
    "input_shape = (1, 28, 28)\n",
    "N_TRAIN_VALID = 1000\n",
    "N_TEST = 200\n",
    "\n",
    "# drop slow mirror from list of MNIST mirrors\n",
    "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
    "                                        if not mirror.startswith(\"http://yann.lecun.com\")]\n",
    "\n",
    "def load(n_train_valid=N_TRAIN_VALID, n_test=N_TEST):\n",
    "    # split between train and test sets\n",
    "    train = torchvision.datasets.MNIST(\"./\", train=True, download=True)\n",
    "    test = torchvision.datasets.MNIST(\"./\", train=False, download=True)\n",
    "    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)\n",
    "    x_train = x_train[:n_train_valid]\n",
    "    y_train = y_train[:n_train_valid]\n",
    "    x_test = x_test[:n_test]\n",
    "    y_test = y_test[:n_test]\n",
    "\n",
    "    # split off a validation set for hyperparameter tuning\n",
    "    train_size = int(n_train_valid * 0.75)\n",
    "    x_train, x_val = x_train[:train_size], x_train[train_size:]\n",
    "    y_train, y_val = y_train[:train_size], y_train[train_size:]\n",
    "\n",
    "    training_set = TensorDataset(x_train, y_train)\n",
    "    validation_set = TensorDataset(x_val, y_val)\n",
    "    test_set = TensorDataset(x_test, y_test)\n",
    "    datasets = [training_set, validation_set, test_set]\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to log these datasets as Artifacts, we just need to:\n",
    "\n",
    "1. Create a Run with `wandb.init`\n",
    "1. Create an Artifact for the dataset\n",
    "1. Save and log the associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdemo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221120_195515-6dupfspf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/6dupfspf\" target=\"_blank\">celestial-thunder-1</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">celestial-thunder-1</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/6dupfspf\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/6dupfspf</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221120_195515-6dupfspf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_and_log(steps):\n",
    "    # start a run, with a type to label it and a project name\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"load-data\") as run:\n",
    "        datasets = load()  # separate code for loading the datasets\n",
    "        names = [\"training\", \"validation\", \"test\"]\n",
    "\n",
    "        # create our Artifact\n",
    "        raw_data = wandb.Artifact(\n",
    "            \"mnist-preprocess\", type=\"dataset\",\n",
    "            description=\"Preprocessed MNIST dataset\",\n",
    "            metadata={\"source\": \"torchvision.datasets.MNIST\",\n",
    "                        \"sizes\": [len(dataset) for dataset in datasets]})\n",
    "\n",
    "        for name, data in zip(names, datasets):\n",
    "            # Store a new file in the artifact, and write data\n",
    "            with raw_data.new_file(name + \".pt\", mode=\"wb\") as file:\n",
    "                processed_dataset = preprocess(data, **steps)\n",
    "                x, y = processed_dataset.tensors\n",
    "                torch.save((x, y), file)\n",
    "\n",
    "        # Save the artifact to Wandb\n",
    "        run.log_artifact(raw_data)\n",
    "\n",
    "def preprocess(dataset, normalize=True, expand_dims=True):\n",
    "    x, y = dataset.tensors\n",
    "    if normalize:\n",
    "        # Scale images to the [0, 1] range\n",
    "        x = x.type(torch.float32) / 255\n",
    "    if expand_dims:\n",
    "        # Make sure images have shape (1, 28, 28)\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "    return TensorDataset(x, y)\n",
    "\n",
    "steps = {\"normalize\": True, \"expand_dims\": True}\n",
    "\n",
    "load_and_log(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wandb.init\n",
    "\n",
    "`Run` defines which Wandb project we want to run.\n",
    "\n",
    "Artifacts that are logged will be kept inside a single Wandb project. This keeps things simple, but Artifacts are portable across projects!\n",
    "\n",
    "To keep track of different types of jobs, it's useful to provide a `job_type` when making Runs. This keeps the graph of your Artifacts nice and tidy.\n",
    "\n",
    "**Note**: the `job_type` should be descriptive and correspond to a single step of your pipeline. Here, we separate out loading data from preprocessing data.\n",
    "\n",
    "<br>\n",
    "\n",
    "### wandb.Artifact\n",
    "\n",
    "To log an Artifact, we make an Artifact object with a name.\n",
    "\n",
    "**Note**: the name should be descriptive, hyphen-separated, and correspond to variable names in the code.\n",
    "\n",
    "An Artifact also has a type. Just like `job_type` for Runs, this is used for organizing the graph of Runs and Artifacts.\n",
    "\n",
    "You can attach a description and some metadata as a dictionary. The metadata needs to be serializable to JSON.\n",
    "\n",
    "<br>\n",
    "\n",
    "### artifact.new_file and run.log_artifact\n",
    "\n",
    "Once we've made an Artifact object, we need to add files to it.\n",
    "\n",
    "Artifacts are structured like directories, with files and sub-directories.\n",
    "\n",
    "We use the `new_file` method to simultaneously write the file and attach it to the Artifact. We also use the `add_file` method, which separates those two steps.\n",
    "\n",
    "Once we've added all of our files, we call `log_artifact` to upload artifacts to wandb.ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example show us how Artifacts can improve your ML workflow.\n",
    "\n",
    "This cell below builds a simple Convolutional Neurnet Net model in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "      def __init__(self, hidden_layer_sizes=[32, 64],\n",
    "            kernel_sizes=[3],\n",
    "            activation=\"ReLU\",\n",
    "            pool_sizes=[2],\n",
    "            dropout=0.5,\n",
    "            num_classes=num_classes,\n",
    "            input_shape=input_shape):\n",
    "            super(ConvNet, self).__init__()\n",
    "\n",
    "            self.layer1 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels=input_shape[0], out_channels=hidden_layer_sizes[0], kernel_size=kernel_sizes[0]),\n",
    "                  getattr(nn, activation)(),\n",
    "                  nn.MaxPool2d(kernel_size=pool_sizes[0])\n",
    "            )\n",
    "            self.layer2 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels=hidden_layer_sizes[0], out_channels=hidden_layer_sizes[-1], kernel_size=kernel_sizes[-1]),\n",
    "                  getattr(nn, activation)(),\n",
    "                  nn.MaxPool2d(kernel_size=pool_sizes[-1])\n",
    "            )\n",
    "            self.layer3 = nn.Sequential(\n",
    "                  nn.Flatten(),\n",
    "                  nn.Dropout(dropout)\n",
    "            )\n",
    "\n",
    "            fc_input_dims = floor((input_shape[1] - kernel_sizes[0] + 1) / pool_sizes[0]) # layer 1 output size\n",
    "            fc_input_dims = floor((fc_input_dims - kernel_sizes[-1] + 1) / pool_sizes[-1]) # layer 2 output size\n",
    "            fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-1] # layer 3 output size\n",
    "\n",
    "            self.fc = nn.Linear(fc_input_dims, num_classes)\n",
    "\n",
    "      def forward(self, x):\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.fc(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, train_loader, valid_loader, config):\n",
    "    optimizer = getattr(torch.optim, config.optimizer)(model.parameters())\n",
    "    model.train()\n",
    "    for epoch in range(config.epochs):\n",
    "        train_epoch(model, train_loader, valid_loader, config.batch_log_interval, optimizer, epoch)\n",
    "\n",
    "def train_epoch(model, train_loader, valid_loader, batch_log_interval, optimizer, epoch):\n",
    "    example_ct = epoch * len(train_loader)\n",
    "    cumu_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        cumu_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        example_ct += len(data)\n",
    "        if batch_idx % batch_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0%})]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                batch_idx / len(train_loader), loss.item()))\n",
    "            train_log(loss, example_ct, epoch)\n",
    "\n",
    "    if not valid_loader is None:\n",
    "        # evaluate the model on the validation set at each epoch\n",
    "        loss, accuracy = test(model, valid_loader)\n",
    "        test_log(loss, accuracy, example_ct, epoch)\n",
    "\n",
    "    return cumu_loss / len(train_loader)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum')  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"train/loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    \n",
    "def test_log(loss, accuracy, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    accuracy = float(accuracy)\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"validation/loss\": loss, \"validation/accuracy\": accuracy}, step=example_ct)\n",
    "    print(f\"Loss/accuracy after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}/{accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run two separate Artifact-producing Runs this time.\n",
    "\n",
    "Once the first finishes training the model, the second will consume the trained model Artifact by evaluating its performance on the `test_dataset`.\n",
    "\n",
    "We also select the most confused 32 examples -- on which the `categorical_crossentropy` is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221120_195521-10pikmf8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/10pikmf8\" target=\"_blank\">polished-blaze-2</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/750 (0%)]\tLoss: 2.332826\n",
      "Loss after 00128 examples: 2.333\n",
      "Train Epoch: 0 [256/750 (33%)]\tLoss: 2.261553\n",
      "Loss after 00384 examples: 2.262\n",
      "Train Epoch: 0 [512/750 (67%)]\tLoss: 2.207235\n",
      "Loss after 00640 examples: 2.207\n",
      "Loss/accuracy after 00750 examples: 2.124/39.200\n",
      "Train Epoch: 1 [0/750 (0%)]\tLoss: 2.054074\n",
      "Loss after 00134 examples: 2.054\n",
      "Train Epoch: 1 [256/750 (33%)]\tLoss: 1.952422\n",
      "Loss after 00390 examples: 1.952\n",
      "Train Epoch: 1 [512/750 (67%)]\tLoss: 1.920000\n",
      "Loss after 00646 examples: 1.920\n",
      "Loss/accuracy after 00756 examples: 1.728/76.000\n",
      "Train Epoch: 2 [0/750 (0%)]\tLoss: 1.615441\n",
      "Loss after 00140 examples: 1.615\n",
      "Train Epoch: 2 [256/750 (33%)]\tLoss: 1.441084\n",
      "Loss after 00396 examples: 1.441\n",
      "Train Epoch: 2 [512/750 (67%)]\tLoss: 1.445229\n",
      "Loss after 00652 examples: 1.445\n",
      "Loss/accuracy after 00762 examples: 1.207/77.600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▅█</td></tr><tr><td>train/loss</td><td>█▄▁</td></tr><tr><td>validation/accuracy</td><td>▁██</td></tr><tr><td>validation/loss</td><td>█▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train/loss</td><td>2.20724</td></tr><tr><td>validation/accuracy</td><td>77.6</td></tr><tr><td>validation/loss</td><td>1.2069</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">polished-blaze-2</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/10pikmf8\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/10pikmf8</a><br/>Synced 7 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221120_195521-10pikmf8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221120_195530-3s1kbw19</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/3s1kbw19\" target=\"_blank\">treasured-silence-3</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07964c5f63e74b4eb3e40f585e50aa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.041 MB of 0.041 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>75.0</td></tr><tr><td>loss</td><td>1.1973</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">treasured-silence-3</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/3s1kbw19\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/3s1kbw19</a><br/>Synced 7 W&B file(s), 32 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221120_195530-3s1kbw19/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    loss, accuracy = test(model, test_loader)\n",
    "    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)\n",
    "    return loss, accuracy, highest_losses, hardest_examples, true_labels, predictions\n",
    "\n",
    "def get_hardest_k_examples(model, testing_set, k=32):\n",
    "    model.eval()\n",
    "    loader = DataLoader(testing_set, 1, shuffle=False)\n",
    "    # get the losses and predictions for each item in the dataset\n",
    "    losses = None\n",
    "    predictions = None\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            if losses is None:\n",
    "                losses = loss.view((1, 1))\n",
    "                predictions = pred\n",
    "            else:\n",
    "                losses = torch.cat((losses, loss.view((1, 1))), 0)\n",
    "                predictions = torch.cat((predictions, pred), 0)\n",
    "\n",
    "    argsort_loss = torch.argsort(losses, dim=0)\n",
    "    highest_k_losses = losses[argsort_loss[-k:]]\n",
    "    hardest_k_examples = testing_set[argsort_loss[-k:]][0]\n",
    "    true_labels = testing_set[argsort_loss[-k:]][1]\n",
    "    predicted_labels = predictions[argsort_loss[-k:]]\n",
    "    return highest_k_losses, hardest_k_examples, true_labels, predicted_labels\n",
    "\n",
    "def train_and_log(model_config, train_config):\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"train\", config=train_config) as run:\n",
    "        train_config = wandb.config\n",
    "        data = run.use_artifact('mnist-preprocess:latest')\n",
    "        data_dir = data.download()\n",
    "\n",
    "        training_dataset = read(data_dir, \"training\")\n",
    "        validation_dataset = read(data_dir, \"validation\")\n",
    "        train_loader = DataLoader(training_dataset, batch_size=train_config.batch_size)\n",
    "        validation_loader = DataLoader(validation_dataset, batch_size=train_config.batch_size)\n",
    "        \n",
    "        train_config.update(model_config)\n",
    "        model = ConvNet(**model_config)\n",
    "        model = model.to(device)\n",
    "        train(model, train_loader, validation_loader, train_config)\n",
    "        \n",
    "        model_artifact = wandb.Artifact(\n",
    "            \"trained-model\", type=\"model\",\n",
    "            description=\"Trained NN model\",\n",
    "            metadata=dict(model_config))\n",
    "\n",
    "        with model_artifact.new_file(\"trained_model.pth\", mode=\"wb\") as file:\n",
    "            torch.save(model.state_dict(), file)\n",
    "\n",
    "        run.log_artifact(model_artifact)\n",
    "\n",
    "    return model\n",
    "    \n",
    "def evaluate_and_log(config=None):\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"report\", config=config) as run:\n",
    "        data = run.use_artifact('mnist-preprocess:latest')\n",
    "        data_dir = data.download()\n",
    "        testing_set = read(data_dir, \"test\")\n",
    "        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=128, shuffle=False)\n",
    "\n",
    "        model_artifact = run.use_artifact(\"trained-model:latest\")\n",
    "        model_dir = model_artifact.download()\n",
    "        model_path = os.path.join(model_dir, \"trained_model.pth\")\n",
    "        model_config = model_artifact.metadata\n",
    "\n",
    "        model = ConvNet(**model_config)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.to(device)\n",
    "\n",
    "        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)\n",
    "        run.summary.update({\"loss\": loss, \"accuracy\": accuracy})\n",
    "\n",
    "        wandb.log({\"high-loss-examples\":\n",
    "            [wandb.Image(hard_example, caption=str(int(pred)) + \",\" +  str(int(label)))\n",
    "                for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})\n",
    "\n",
    "def read(data_dir, ds_name):\n",
    "    filename = ds_name + \".pt\"\n",
    "    x, y = torch.load(os.path.join(data_dir, filename))\n",
    "    return TensorDataset(x, y)\n",
    "\n",
    "model_config = {\"hidden_layer_sizes\": [32, 64],\n",
    "                \"kernel_sizes\": [3],\n",
    "                \"activation\": \"ReLU\",\n",
    "                \"pool_sizes\": [2],\n",
    "                \"dropout\": 0.5,\n",
    "                \"num_classes\": 10}\n",
    "\n",
    "train_config = {\"batch_size\": 128,\n",
    "                \"epochs\": 3,\n",
    "                \"batch_log_interval\": 2,\n",
    "                \"optimizer\": \"Adam\"}\n",
    "\n",
    "model = train_and_log(model_config, train_config)\n",
    "evaluate_and_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run.use_artifact\n",
    "\n",
    "To use an Artifact, we need to know its name and its version.\n",
    "\n",
    "By default, the last uploaded version is tagged as `latest`. Versions are separated from names with `:`, so the Artifact we want is `mnist-preprocess:latest`.\n",
    "\n",
    "<br>\n",
    "\n",
    "### artifact.download\n",
    "\n",
    "Before we actually download anything, we check to see if the right version is available locally by using *hashing*.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: the steps of the preprocessing are saved with the `preprocessed_data` as metadata.\n",
    "\n",
    "If you're trying to make your experiments reproducible, capturing lots of metadata is a good idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning\n",
    "\n",
    "In Wandb, *Hyperparameter Sweeps* provide an organized and efficient way to search through high dimensional hyperparameter spaces to find the most performant model.\n",
    "\n",
    "They enable this by automatically searching through combinations of hyperparameter values (e.g. learning rate, batch size, number of hidden layers, optimizer type) to find the most optimal values.\n",
    "\n",
    "![wandb-sweep-overview](assets/wandb-sweep-overview.png)\n",
    "\n",
    "To run a hyperparameter sweep with Wandb, there are 3 simple steps:\n",
    "\n",
    "1.  Define the sweep configuration\n",
    "\n",
    "    We do this by creating a dictionary that specifies the search strategy, optimization metric, and parameters to search through.\n",
    "\n",
    "1.  Initialize the sweep\n",
    "    \n",
    "    We initialize the sweep and pass in the dictionary of sweep configurations\n",
    "\n",
    "    ```bash\n",
    "    sweep_id = wandb.sweep(sweep_config)\n",
    "    ```\n",
    "\n",
    "1.  Run the sweep agent\n",
    "    \n",
    "    We call `wandb.agent()` and pass the `sweep_id` to run, along with a function that defines your training steps:\n",
    "\n",
    "    ```bash\n",
    "    wandb.agent(sweep_id, function=train)\n",
    "    ```\n",
    "\n",
    "In this section, we'll see how you can run sophisticated hyperparameter sweeps using Wandb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Define Sweep config\n",
    "\n",
    "A Sweep combines a strategy for trying out a bunch of hyperparameter values with the code that evalutes them.\n",
    "\n",
    "#### Pick a method\n",
    "The first thing we need to define is the method for choosing new parameter values. It can be:\n",
    "\n",
    "- `grid` Search – Iterate over every combination of hyperparameter values. Very effective, but can be computationally costly.\n",
    "- `random` Search – Select each new combination at random according to provided distributions. Surprisingly effective!\n",
    "- `bayesian` Search – Create a probabilistic model of metric score as a function of the hyperparameters, and choose parameters with high probability of improving the metric. Works well for small numbers of continuous parameters but scales poorly.\n",
    "\n",
    "We select `random` Search for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've picked a method to try out new values of the hyperparameters, you need to define what those parameters are.\n",
    "\n",
    "This step is straightforward: just give the parameter a name and specify a list of legal values of the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'random',\n",
       " 'parameters': {'epochs': {'value': 1},\n",
       "  'optimizer': {'values': ['adam', 'sgd']},\n",
       "  'hidden_layer_1_size': {'values': [16, 32]},\n",
       "  'hidden_layer_2_size': {'values': [32, 64]},\n",
       "  'dropout': {'values': [0.4, 0.5]},\n",
       "  'learning_rate': {'distribution': 'uniform', 'min': 0, 'max': 0.1},\n",
       "  'batch_size': {'distribution': 'q_log_uniform_values',\n",
       "   'q': 8,\n",
       "   'min': 32,\n",
       "   'max': 256}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    # epochs var doesn't vary, but we still want it here\n",
    "    'epochs': {\n",
    "        'value': 1,\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd'],\n",
    "    },\n",
    "    'hidden_layer_1_size': {\n",
    "        'values': [16, 32],\n",
    "    },\n",
    "    'hidden_layer_2_size': {\n",
    "        'values': [32, 64],\n",
    "    },\n",
    "    'dropout': {\n",
    "        'values': [0.4, 0.5],\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0,\n",
    "        'max': 0.1\n",
    "    },\n",
    "    'batch_size': {\n",
    "        # integers between 32 and 256\n",
    "        # with evenly-distributed logarithms \n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q': 8,\n",
    "        'min': 32,\n",
    "        'max': 256,\n",
    "    }\n",
    "}\n",
    "sweep_config['parameters'] = parameters\n",
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wandb also offers the option to `early_terminate` your runs with the `HyperBand` scheduling algorithm. See more [here](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#early_terminate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Run the Sweep\n",
    "\n",
    "The Sweep Controller is in charge of our Sweep. The Sweep Controller instructs how to run each set of hyperparameters via Agents.\n",
    "\n",
    "In a typical Sweep, the Controller lives on Wandb's server, while the agents who complete runs live on your machine(s). This makes it easy to scale up Sweeps by just adding more machines to run agents!\n",
    "\n",
    "![sweeps-diagram](assets/sweeps-diagram.png)\n",
    "\n",
    "We can initialize a Sweep Controller by calling `wandb.sweep` with `sweep_config` and project name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: f2d0sj4o\n",
      "Sweep URL: http://localhost:8080/demo/soict-2022/sweeps/f2d0sj4o\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can actually execute the sweep, we need to define the training procedure used in the Sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_log_interval = 2\n",
    "\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        loader = build_dataset(run, config)\n",
    "        model = build_model(run, config)\n",
    "        optimizer = build_optimizer(model, config)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_loss = train_epoch(model, loader, None, batch_log_interval, optimizer, epoch)\n",
    "            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines: `build_dataset`, `build_model`, and `build_optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(run, config):\n",
    "    batch_size = config.batch_size\n",
    "    data = run.use_artifact('mnist-preprocess:latest')\n",
    "    data_dir = data.download()\n",
    "    training_dataset = read(data_dir, \"training\")\n",
    "    sub_dataset = torch.utils.data.Subset(\n",
    "        training_dataset, indices=range(0, len(training_dataset), 5))\n",
    "    train_loader = DataLoader(sub_dataset, batch_size=batch_size)\n",
    "    return train_loader\n",
    "\n",
    "def build_model(run, config):\n",
    "    model_config = {\n",
    "        'hidden_layer_sizes': [\n",
    "            config.hidden_layer_1_size,\n",
    "            config.hidden_layer_2_size,\n",
    "        ],\n",
    "        'dropout': config.dropout,\n",
    "    }\n",
    "    model = ConvNet(**model_config)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "        \n",
    "def build_optimizer(model, config):\n",
    "    optimizer = config.optimizer\n",
    "    learning_rate = config.learning_rate\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will launch an agent that runs train 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wh3o1fbb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.09373099134024647\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221120_195541-wh3o1fbb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/wh3o1fbb\" target=\"_blank\">fanciful-sweep-1</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022/sweeps/f2d0sj4o\" target=\"_blank\">http://localhost:8080/demo/soict-2022/sweeps/f2d0sj4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/150 (0%)]\tLoss: 2.309748\n",
      "Loss after 00128 examples: 2.310\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9339c3b2be423c9d77d9b5c023a4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>33.38466</td></tr><tr><td>train/loss</td><td>2.30975</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fanciful-sweep-1</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/wh3o1fbb\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/wh3o1fbb</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221120_195541-wh3o1fbb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: grqs46wh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.02563970103436769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221120_195555-grqs46wh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/grqs46wh\" target=\"_blank\">stilted-sweep-2</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022/sweeps/f2d0sj4o\" target=\"_blank\">http://localhost:8080/demo/soict-2022/sweeps/f2d0sj4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/150 (0%)]\tLoss: 2.307336\n",
      "Loss after 00048 examples: 2.307\n",
      "Train Epoch: 0 [96/150 (50%)]\tLoss: 2.287161\n",
      "Loss after 00144 examples: 2.287\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e95418cc5824cde9e2cef90aede7c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>2.30805</td></tr><tr><td>train/loss</td><td>2.28716</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">stilted-sweep-2</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/grqs46wh\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/grqs46wh</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221120_195555-grqs46wh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9v8aq57l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 208\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.023653073963399208\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221120_195606-9v8aq57l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/9v8aq57l\" target=\"_blank\">copper-sweep-3</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022/sweeps/f2d0sj4o\" target=\"_blank\">http://localhost:8080/demo/soict-2022/sweeps/f2d0sj4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/150 (0%)]\tLoss: 2.306984\n",
      "Loss after 00150 examples: 2.307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43ef49dcd2d495ea057a917915ca18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>2.30698</td></tr><tr><td>train/loss</td><td>2.30698</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">copper-sweep-3</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/9v8aq57l\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/9v8aq57l</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221120_195606-9v8aq57l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9kxfkpli with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.039848386304843275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221120_195617-9kxfkpli</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/9kxfkpli\" target=\"_blank\">fearless-sweep-4</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022/sweeps/f2d0sj4o\" target=\"_blank\">http://localhost:8080/demo/soict-2022/sweeps/f2d0sj4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/150 (0%)]\tLoss: 2.306762\n",
      "Loss after 00040 examples: 2.307\n",
      "Train Epoch: 0 [80/150 (50%)]\tLoss: 2.252375\n",
      "Loss after 00120 examples: 2.252\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f99ba086d7847de89c688eecfcb676c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>2.29722</td></tr><tr><td>train/loss</td><td>2.25237</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fearless-sweep-4</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/9kxfkpli\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/9kxfkpli</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221120_195617-9kxfkpli/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9v4z3xwn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.019880191611981027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221120_195630-9v4z3xwn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022/runs/9v4z3xwn\" target=\"_blank\">lucky-sweep-5</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022/sweeps/f2d0sj4o\" target=\"_blank\">http://localhost:8080/demo/soict-2022/sweeps/f2d0sj4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/150 (0%)]\tLoss: 2.306142\n",
      "Loss after 00032 examples: 2.306\n",
      "Train Epoch: 0 [64/150 (40%)]\tLoss: 2.287060\n",
      "Loss after 00096 examples: 2.287\n",
      "Train Epoch: 0 [88/150 (80%)]\tLoss: 2.258342\n",
      "Loss after 00150 examples: 2.258\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2026e83399ca4dc7ad36b0591a062d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>train/loss</td><td>█▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>2.2897</td></tr><tr><td>train/loss</td><td>2.25834</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lucky-sweep-5</strong>: <a href=\"http://localhost:8080/demo/soict-2022/runs/9v4z3xwn\" target=\"_blank\">http://localhost:8080/demo/soict-2022/runs/9v4z3xwn</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221120_195630-9v4z3xwn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train_sweep, count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Visualize Sweep Results\n",
    "\n",
    "#### Parallel Coordinates Plot\n",
    "\n",
    "This plot maps hyperparameter values to model metrics.\n",
    "\n",
    "![hyperparam-plot](assets/hyperparam-plot.png)\n",
    "\n",
    "#### Hyperparameter Importance Plot\n",
    "\n",
    "The hyperparameter importance plot surfaces which hyperparameters were the best predictors of your metrics. Wandb reports feature importance (using a random forest model) and correlation (using a linear model).\n",
    "\n",
    "![hyperparam-importance](assets/hyperparam-importance.png)\n",
    "\n",
    "These visualizations can help you save both time and resources running expensive hyperparameter optimizations by refining the parameters (and value ranges), and thereby worthy of further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Stop the Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Stopping sweep demo/soict-2022/f2d0sj4o.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Done.\n"
     ]
    }
   ],
   "source": [
    "# For self-hosted Wandb server\n",
    "!wandb sweep --stop \"demo/$PROJECT_NAME/$sweep_id\"\n",
    "\n",
    "# For Wandb cloud server\n",
    "# Stop sweep at https://<wandb-server-address>/<wandb-user>/soict-2022/sweeps/<sweep-id>/controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('soict')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "291bacc89d47c570d681c8cb8133df5aeea2ebc003da29abf8c072691acbbc14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
