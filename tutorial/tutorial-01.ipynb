{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "In this notebook, I'll show you how to use Wandb to perform the following tasks in a typical ML workflow:\n",
    "\n",
    "1. Data versioning\n",
    "1. Experiment tracking\n",
    "1. Hyperparameter tuning\n",
    "\n",
    "In order to run this notebook, please follow the instruction in the `README.md` file to setup your working environment with Wandb installed and your Wandb account logged in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_NAME = \"soict-2022-heavy\"\n",
    "# N_TRAIN_VALID = 10000\n",
    "# N_TEST = int(N_TRAIN_VALID * 0.2)\n",
    "# TRAIN_EPOCH = 5\n",
    "# SWEEP_N_TRAIN = 100\n",
    "# SWEEP_EPOCH = 5\n",
    "# SWEEP_COUNT = 15\n",
    "\n",
    "PROJECT_NAME = \"soict-2022-demo\"\n",
    "N_TRAIN_VALID = 1000\n",
    "N_TEST = int(N_TRAIN_VALID * 0.2)\n",
    "TRAIN_EPOCH = 3\n",
    "SWEEP_N_TRAIN = 5\n",
    "SWEEP_EPOCH = 1\n",
    "SWEEP_COUNT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data versioning\n",
    "\n",
    "> Those that fail to learn from history are doomed to repeat it. - Winston Churchill\n",
    "\n",
    "In Wandb, an `Artifact` is the input or output of a process. A `Run` is a task that we want to perform.\n",
    "\n",
    "In ML, the most important artifacts are _datasets_ and _models_. They should be organized so that you can learn from them.\n",
    "\n",
    "In Wandb, we can log `Artifact` as ouputs of Wandb `Run`s or use `Artifact` as input to `Run`s, as in this diagram, where a training run takes in a dataset and produces a model.\n",
    "\n",
    "![wandb-artifact-run](assets/wandb-artifact-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses the MNIST database. The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "\n",
    "![mnist-examples](assets/mnist-exmaples.png)\n",
    "\n",
    "We start with the `Dataset`s:\n",
    "\n",
    "- A training set and a validation set, for model training\n",
    "- A test set, for model evaluation\n",
    "\n",
    "The cell below defines these three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data parameters\n",
    "num_classes = 10\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "# drop slow mirror from list of MNIST mirrors\n",
    "torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
    "                                        if not mirror.startswith(\"http://yann.lecun.com\")]\n",
    "\n",
    "def load(n_train_valid=N_TRAIN_VALID, n_test=N_TEST):\n",
    "    # split between train and test sets\n",
    "    train = torchvision.datasets.MNIST(\"./\", train=True, download=True)\n",
    "    test = torchvision.datasets.MNIST(\"./\", train=False, download=True)\n",
    "    (x_train, y_train), (x_test, y_test) = (train.data, train.targets), (test.data, test.targets)\n",
    "    x_train = x_train[:n_train_valid]\n",
    "    y_train = y_train[:n_train_valid]\n",
    "    x_test = x_test[:n_test]\n",
    "    y_test = y_test[:n_test]\n",
    "\n",
    "    # split off a validation set for hyperparameter tuning\n",
    "    train_size = int(n_train_valid * 0.75)\n",
    "    x_train, x_val = x_train[:train_size], x_train[train_size:]\n",
    "    y_train, y_val = y_train[:train_size], y_train[train_size:]\n",
    "\n",
    "    training_set = TensorDataset(x_train, y_train)\n",
    "    validation_set = TensorDataset(x_val, y_val)\n",
    "    test_set = TensorDataset(x_test, y_test)\n",
    "    datasets = [training_set, validation_set, test_set]\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to log these datasets as Artifacts, we just need to:\n",
    "\n",
    "1. Create a Run with `wandb.init`\n",
    "1. Create an Artifact for the dataset\n",
    "1. Save and log the associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdemo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154508-2pa5jyo0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/2pa5jyo0\" target=\"_blank\">absurd-music-1</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abd568e47984759814c1cf543a5915b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='37.026 MB of 37.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">absurd-music-1</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/2pa5jyo0\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/2pa5jyo0</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154508-2pa5jyo0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_and_log(steps):\n",
    "    # start a run, with a type to label it and a project name\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"load-data\") as run:\n",
    "        datasets = load()  # separate code for loading the datasets\n",
    "        names = [\"training\", \"validation\", \"test\"]\n",
    "\n",
    "        # create our Artifact\n",
    "        raw_data = wandb.Artifact(\n",
    "            \"mnist-preprocess\", type=\"dataset\",\n",
    "            description=\"Preprocessed MNIST dataset\",\n",
    "            metadata={\"source\": \"torchvision.datasets.MNIST\",\n",
    "                        \"sizes\": [len(dataset) for dataset in datasets]})\n",
    "\n",
    "        for name, data in zip(names, datasets):\n",
    "            # Store a new file in the artifact, and write data\n",
    "            with raw_data.new_file(name + \".pt\", mode=\"wb\") as file:\n",
    "                processed_dataset = preprocess(data, **steps)\n",
    "                x, y = processed_dataset.tensors\n",
    "                torch.save((x, y), file)\n",
    "\n",
    "        # Save the artifact to Wandb\n",
    "        run.log_artifact(raw_data)\n",
    "\n",
    "def preprocess(dataset, normalize=True, expand_dims=True):\n",
    "    x, y = dataset.tensors\n",
    "    if normalize:\n",
    "        # Scale images to the [0, 1] range\n",
    "        x = x.type(torch.float32) / 255\n",
    "    if expand_dims:\n",
    "        # Make sure images have shape (1, 28, 28)\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "    return TensorDataset(x, y)\n",
    "\n",
    "steps = {\"normalize\": True, \"expand_dims\": True}\n",
    "\n",
    "load_and_log(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wandb.init\n",
    "\n",
    "`Run` defines which Wandb project we want to run.\n",
    "\n",
    "Artifacts that are logged will be kept inside a single Wandb project. This keeps things simple, but Artifacts are portable across projects!\n",
    "\n",
    "To keep track of different types of jobs, it's useful to provide a `job_type` when making Runs. This keeps the graph of your Artifacts nice and tidy.\n",
    "\n",
    "**Note**: the `job_type` should be descriptive and correspond to a single step of your pipeline. Here, we separate out loading data from preprocessing data.\n",
    "\n",
    "<br>\n",
    "\n",
    "### wandb.Artifact\n",
    "\n",
    "To log an Artifact, we make an Artifact object with a name.\n",
    "\n",
    "**Note**: the name should be descriptive, hyphen-separated, and correspond to variable names in the code.\n",
    "\n",
    "An Artifact also has a type. Just like `job_type` for Runs, this is used for organizing the graph of Runs and Artifacts.\n",
    "\n",
    "You can attach a description and some metadata as a dictionary. The metadata needs to be serializable to JSON.\n",
    "\n",
    "<br>\n",
    "\n",
    "### artifact.new_file and run.log_artifact\n",
    "\n",
    "Once we've made an Artifact object, we need to add files to it.\n",
    "\n",
    "Artifacts are structured like directories, with files and sub-directories.\n",
    "\n",
    "We use the `new_file` method to simultaneously write the file and attach it to the Artifact. We also use the `add_file` method, which separates those two steps.\n",
    "\n",
    "Once we've added all of our files, we call `log_artifact` to upload artifacts to wandb.ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example show us how Artifacts can improve your ML workflow.\n",
    "\n",
    "This cell below builds a simple Convolutional Neurnet Net model in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "      def __init__(self, hidden_layer_sizes=[32, 64],\n",
    "            kernel_sizes=[3],\n",
    "            activation=\"ReLU\",\n",
    "            pool_sizes=[2],\n",
    "            dropout=0.5,\n",
    "            num_classes=num_classes,\n",
    "            input_shape=input_shape):\n",
    "            super(ConvNet, self).__init__()\n",
    "\n",
    "            self.layer1 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels=input_shape[0], out_channels=hidden_layer_sizes[0], kernel_size=kernel_sizes[0]),\n",
    "                  getattr(nn, activation)(),\n",
    "                  nn.MaxPool2d(kernel_size=pool_sizes[0])\n",
    "            )\n",
    "            self.layer2 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels=hidden_layer_sizes[0], out_channels=hidden_layer_sizes[-1], kernel_size=kernel_sizes[-1]),\n",
    "                  getattr(nn, activation)(),\n",
    "                  nn.MaxPool2d(kernel_size=pool_sizes[-1])\n",
    "            )\n",
    "            self.layer3 = nn.Sequential(\n",
    "                  nn.Flatten(),\n",
    "                  nn.Dropout(dropout)\n",
    "            )\n",
    "\n",
    "            fc_input_dims = floor((input_shape[1] - kernel_sizes[0] + 1) / pool_sizes[0]) # layer 1 output size\n",
    "            fc_input_dims = floor((fc_input_dims - kernel_sizes[-1] + 1) / pool_sizes[-1]) # layer 2 output size\n",
    "            fc_input_dims = fc_input_dims*fc_input_dims*hidden_layer_sizes[-1] # layer 3 output size\n",
    "\n",
    "            self.fc = nn.Linear(fc_input_dims, num_classes)\n",
    "\n",
    "      def forward(self, x):\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.fc(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model, train_loader, valid_loader, config):\n",
    "    optimizer = getattr(torch.optim, config.optimizer)(model.parameters())\n",
    "    model.train()\n",
    "    for epoch in range(config.epochs):\n",
    "        train_epoch(model, train_loader, valid_loader, config.batch_log_interval, optimizer, epoch)\n",
    "\n",
    "def train_epoch(model, train_loader, valid_loader, batch_log_interval, optimizer, epoch):\n",
    "    example_ct = epoch * len(train_loader.dataset)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        example_ct += len(data)\n",
    "        if batch_idx % batch_log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0%})]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                batch_idx / len(train_loader), loss.item()))\n",
    "            train_log(loss, example_ct, epoch)\n",
    "\n",
    "    if not valid_loader is None:\n",
    "        # evaluate the model on the validation set at each epoch\n",
    "        loss, accuracy = test(model, valid_loader)\n",
    "        test_log(loss, accuracy, example_ct, epoch)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum')  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"train/loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    \n",
    "def test_log(loss, accuracy, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "    accuracy = float(accuracy)\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"validation/loss\": loss, \"validation/accuracy\": accuracy}, step=example_ct)\n",
    "    print(f\"Loss/accuracy after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}/{accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run two separate Artifact-producing Runs this time.\n",
    "\n",
    "Once the first finishes training the model, the second will consume the trained model Artifact by evaluating its performance on the `test_dataset`.\n",
    "\n",
    "We also select the most confused 32 examples -- on which the `categorical_crossentropy` is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3090ab70871a42ac9ea603fb7778d2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016674064883333332, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154514-23718yto</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/23718yto\" target=\"_blank\">stellar-voice-2</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/7500 (0%)]\tLoss: 2.332069\n",
      "Loss after 00064 examples: 2.332\n",
      "Train Epoch: 0 [320/7500 (4%)]\tLoss: 2.191033\n",
      "Loss after 00384 examples: 2.191\n",
      "Train Epoch: 0 [640/7500 (8%)]\tLoss: 1.963947\n",
      "Loss after 00704 examples: 1.964\n",
      "Train Epoch: 0 [960/7500 (13%)]\tLoss: 1.778343\n",
      "Loss after 01024 examples: 1.778\n",
      "Train Epoch: 0 [1280/7500 (17%)]\tLoss: 1.456213\n",
      "Loss after 01344 examples: 1.456\n",
      "Train Epoch: 0 [1600/7500 (21%)]\tLoss: 1.019487\n",
      "Loss after 01664 examples: 1.019\n",
      "Train Epoch: 0 [1920/7500 (25%)]\tLoss: 0.900709\n",
      "Loss after 01984 examples: 0.901\n",
      "Train Epoch: 0 [2240/7500 (30%)]\tLoss: 0.751033\n",
      "Loss after 02304 examples: 0.751\n",
      "Train Epoch: 0 [2560/7500 (34%)]\tLoss: 0.540599\n",
      "Loss after 02624 examples: 0.541\n",
      "Train Epoch: 0 [2880/7500 (38%)]\tLoss: 0.716803\n",
      "Loss after 02944 examples: 0.717\n",
      "Train Epoch: 0 [3200/7500 (42%)]\tLoss: 0.559175\n",
      "Loss after 03264 examples: 0.559\n",
      "Train Epoch: 0 [3520/7500 (47%)]\tLoss: 0.598229\n",
      "Loss after 03584 examples: 0.598\n",
      "Train Epoch: 0 [3840/7500 (51%)]\tLoss: 0.323912\n",
      "Loss after 03904 examples: 0.324\n",
      "Train Epoch: 0 [4160/7500 (55%)]\tLoss: 0.409423\n",
      "Loss after 04224 examples: 0.409\n",
      "Train Epoch: 0 [4480/7500 (59%)]\tLoss: 0.457928\n",
      "Loss after 04544 examples: 0.458\n",
      "Train Epoch: 0 [4800/7500 (64%)]\tLoss: 0.672244\n",
      "Loss after 04864 examples: 0.672\n",
      "Train Epoch: 0 [5120/7500 (68%)]\tLoss: 0.670459\n",
      "Loss after 05184 examples: 0.670\n",
      "Train Epoch: 0 [5440/7500 (72%)]\tLoss: 0.235303\n",
      "Loss after 05504 examples: 0.235\n",
      "Train Epoch: 0 [5760/7500 (76%)]\tLoss: 0.257500\n",
      "Loss after 05824 examples: 0.257\n",
      "Train Epoch: 0 [6080/7500 (81%)]\tLoss: 0.240093\n",
      "Loss after 06144 examples: 0.240\n",
      "Train Epoch: 0 [6400/7500 (85%)]\tLoss: 0.351698\n",
      "Loss after 06464 examples: 0.352\n",
      "Train Epoch: 0 [6720/7500 (89%)]\tLoss: 0.129120\n",
      "Loss after 06784 examples: 0.129\n",
      "Train Epoch: 0 [7040/7500 (93%)]\tLoss: 0.275028\n",
      "Loss after 07104 examples: 0.275\n",
      "Train Epoch: 0 [7360/7500 (97%)]\tLoss: 0.290033\n",
      "Loss after 07424 examples: 0.290\n",
      "Loss/accuracy after 07500 examples: 0.296/91.560\n",
      "Train Epoch: 1 [0/7500 (0%)]\tLoss: 0.309093\n",
      "Loss after 07564 examples: 0.309\n",
      "Train Epoch: 1 [320/7500 (4%)]\tLoss: 0.143278\n",
      "Loss after 07884 examples: 0.143\n",
      "Train Epoch: 1 [640/7500 (8%)]\tLoss: 0.179012\n",
      "Loss after 08204 examples: 0.179\n",
      "Train Epoch: 1 [960/7500 (13%)]\tLoss: 0.239579\n",
      "Loss after 08524 examples: 0.240\n",
      "Train Epoch: 1 [1280/7500 (17%)]\tLoss: 0.202747\n",
      "Loss after 08844 examples: 0.203\n",
      "Train Epoch: 1 [1600/7500 (21%)]\tLoss: 0.201085\n",
      "Loss after 09164 examples: 0.201\n",
      "Train Epoch: 1 [1920/7500 (25%)]\tLoss: 0.163395\n",
      "Loss after 09484 examples: 0.163\n",
      "Train Epoch: 1 [2240/7500 (30%)]\tLoss: 0.105694\n",
      "Loss after 09804 examples: 0.106\n",
      "Train Epoch: 1 [2560/7500 (34%)]\tLoss: 0.167565\n",
      "Loss after 10124 examples: 0.168\n",
      "Train Epoch: 1 [2880/7500 (38%)]\tLoss: 0.174267\n",
      "Loss after 10444 examples: 0.174\n",
      "Train Epoch: 1 [3200/7500 (42%)]\tLoss: 0.126957\n",
      "Loss after 10764 examples: 0.127\n",
      "Train Epoch: 1 [3520/7500 (47%)]\tLoss: 0.140670\n",
      "Loss after 11084 examples: 0.141\n",
      "Train Epoch: 1 [3840/7500 (51%)]\tLoss: 0.116333\n",
      "Loss after 11404 examples: 0.116\n",
      "Train Epoch: 1 [4160/7500 (55%)]\tLoss: 0.221204\n",
      "Loss after 11724 examples: 0.221\n",
      "Train Epoch: 1 [4480/7500 (59%)]\tLoss: 0.222455\n",
      "Loss after 12044 examples: 0.222\n",
      "Train Epoch: 1 [4800/7500 (64%)]\tLoss: 0.130070\n",
      "Loss after 12364 examples: 0.130\n",
      "Train Epoch: 1 [5120/7500 (68%)]\tLoss: 0.230778\n",
      "Loss after 12684 examples: 0.231\n",
      "Train Epoch: 1 [5440/7500 (72%)]\tLoss: 0.079089\n",
      "Loss after 13004 examples: 0.079\n",
      "Train Epoch: 1 [5760/7500 (76%)]\tLoss: 0.147829\n",
      "Loss after 13324 examples: 0.148\n",
      "Train Epoch: 1 [6080/7500 (81%)]\tLoss: 0.141168\n",
      "Loss after 13644 examples: 0.141\n",
      "Train Epoch: 1 [6400/7500 (85%)]\tLoss: 0.173660\n",
      "Loss after 13964 examples: 0.174\n",
      "Train Epoch: 1 [6720/7500 (89%)]\tLoss: 0.071941\n",
      "Loss after 14284 examples: 0.072\n",
      "Train Epoch: 1 [7040/7500 (93%)]\tLoss: 0.127946\n",
      "Loss after 14604 examples: 0.128\n",
      "Train Epoch: 1 [7360/7500 (97%)]\tLoss: 0.112539\n",
      "Loss after 14924 examples: 0.113\n",
      "Loss/accuracy after 15000 examples: 0.193/94.600\n",
      "Train Epoch: 2 [0/7500 (0%)]\tLoss: 0.155221\n",
      "Loss after 15064 examples: 0.155\n",
      "Train Epoch: 2 [320/7500 (4%)]\tLoss: 0.056501\n",
      "Loss after 15384 examples: 0.057\n",
      "Train Epoch: 2 [640/7500 (8%)]\tLoss: 0.106928\n",
      "Loss after 15704 examples: 0.107\n",
      "Train Epoch: 2 [960/7500 (13%)]\tLoss: 0.148140\n",
      "Loss after 16024 examples: 0.148\n",
      "Train Epoch: 2 [1280/7500 (17%)]\tLoss: 0.128929\n",
      "Loss after 16344 examples: 0.129\n",
      "Train Epoch: 2 [1600/7500 (21%)]\tLoss: 0.142561\n",
      "Loss after 16664 examples: 0.143\n",
      "Train Epoch: 2 [1920/7500 (25%)]\tLoss: 0.126614\n",
      "Loss after 16984 examples: 0.127\n",
      "Train Epoch: 2 [2240/7500 (30%)]\tLoss: 0.058914\n",
      "Loss after 17304 examples: 0.059\n",
      "Train Epoch: 2 [2560/7500 (34%)]\tLoss: 0.126849\n",
      "Loss after 17624 examples: 0.127\n",
      "Train Epoch: 2 [2880/7500 (38%)]\tLoss: 0.112014\n",
      "Loss after 17944 examples: 0.112\n",
      "Train Epoch: 2 [3200/7500 (42%)]\tLoss: 0.071767\n",
      "Loss after 18264 examples: 0.072\n",
      "Train Epoch: 2 [3520/7500 (47%)]\tLoss: 0.082100\n",
      "Loss after 18584 examples: 0.082\n",
      "Train Epoch: 2 [3840/7500 (51%)]\tLoss: 0.077228\n",
      "Loss after 18904 examples: 0.077\n",
      "Train Epoch: 2 [4160/7500 (55%)]\tLoss: 0.148590\n",
      "Loss after 19224 examples: 0.149\n",
      "Train Epoch: 2 [4480/7500 (59%)]\tLoss: 0.138243\n",
      "Loss after 19544 examples: 0.138\n",
      "Train Epoch: 2 [4800/7500 (64%)]\tLoss: 0.085840\n",
      "Loss after 19864 examples: 0.086\n",
      "Train Epoch: 2 [5120/7500 (68%)]\tLoss: 0.131022\n",
      "Loss after 20184 examples: 0.131\n",
      "Train Epoch: 2 [5440/7500 (72%)]\tLoss: 0.046995\n",
      "Loss after 20504 examples: 0.047\n",
      "Train Epoch: 2 [5760/7500 (76%)]\tLoss: 0.118131\n",
      "Loss after 20824 examples: 0.118\n",
      "Train Epoch: 2 [6080/7500 (81%)]\tLoss: 0.123507\n",
      "Loss after 21144 examples: 0.124\n",
      "Train Epoch: 2 [6400/7500 (85%)]\tLoss: 0.137810\n",
      "Loss after 21464 examples: 0.138\n",
      "Train Epoch: 2 [6720/7500 (89%)]\tLoss: 0.043503\n",
      "Loss after 21784 examples: 0.044\n",
      "Train Epoch: 2 [7040/7500 (93%)]\tLoss: 0.111514\n",
      "Loss after 22104 examples: 0.112\n",
      "Train Epoch: 2 [7360/7500 (97%)]\tLoss: 0.081251\n",
      "Loss after 22424 examples: 0.081\n",
      "Loss/accuracy after 22500 examples: 0.161/95.440\n",
      "Train Epoch: 3 [0/7500 (0%)]\tLoss: 0.076958\n",
      "Loss after 22564 examples: 0.077\n",
      "Train Epoch: 3 [320/7500 (4%)]\tLoss: 0.032744\n",
      "Loss after 22884 examples: 0.033\n",
      "Train Epoch: 3 [640/7500 (8%)]\tLoss: 0.082347\n",
      "Loss after 23204 examples: 0.082\n",
      "Train Epoch: 3 [960/7500 (13%)]\tLoss: 0.109915\n",
      "Loss after 23524 examples: 0.110\n",
      "Train Epoch: 3 [1280/7500 (17%)]\tLoss: 0.095790\n",
      "Loss after 23844 examples: 0.096\n",
      "Train Epoch: 3 [1600/7500 (21%)]\tLoss: 0.127180\n",
      "Loss after 24164 examples: 0.127\n",
      "Train Epoch: 3 [1920/7500 (25%)]\tLoss: 0.104739\n",
      "Loss after 24484 examples: 0.105\n",
      "Train Epoch: 3 [2240/7500 (30%)]\tLoss: 0.036139\n",
      "Loss after 24804 examples: 0.036\n",
      "Train Epoch: 3 [2560/7500 (34%)]\tLoss: 0.101356\n",
      "Loss after 25124 examples: 0.101\n",
      "Train Epoch: 3 [2880/7500 (38%)]\tLoss: 0.088379\n",
      "Loss after 25444 examples: 0.088\n",
      "Train Epoch: 3 [3200/7500 (42%)]\tLoss: 0.049963\n",
      "Loss after 25764 examples: 0.050\n",
      "Train Epoch: 3 [3520/7500 (47%)]\tLoss: 0.058711\n",
      "Loss after 26084 examples: 0.059\n",
      "Train Epoch: 3 [3840/7500 (51%)]\tLoss: 0.053237\n",
      "Loss after 26404 examples: 0.053\n",
      "Train Epoch: 3 [4160/7500 (55%)]\tLoss: 0.103474\n",
      "Loss after 26724 examples: 0.103\n",
      "Train Epoch: 3 [4480/7500 (59%)]\tLoss: 0.085223\n",
      "Loss after 27044 examples: 0.085\n",
      "Train Epoch: 3 [4800/7500 (64%)]\tLoss: 0.066354\n",
      "Loss after 27364 examples: 0.066\n",
      "Train Epoch: 3 [5120/7500 (68%)]\tLoss: 0.087930\n",
      "Loss after 27684 examples: 0.088\n",
      "Train Epoch: 3 [5440/7500 (72%)]\tLoss: 0.028332\n",
      "Loss after 28004 examples: 0.028\n",
      "Train Epoch: 3 [5760/7500 (76%)]\tLoss: 0.101393\n",
      "Loss after 28324 examples: 0.101\n",
      "Train Epoch: 3 [6080/7500 (81%)]\tLoss: 0.126648\n",
      "Loss after 28644 examples: 0.127\n",
      "Train Epoch: 3 [6400/7500 (85%)]\tLoss: 0.123345\n",
      "Loss after 28964 examples: 0.123\n",
      "Train Epoch: 3 [6720/7500 (89%)]\tLoss: 0.028064\n",
      "Loss after 29284 examples: 0.028\n",
      "Train Epoch: 3 [7040/7500 (93%)]\tLoss: 0.103770\n",
      "Loss after 29604 examples: 0.104\n",
      "Train Epoch: 3 [7360/7500 (97%)]\tLoss: 0.060901\n",
      "Loss after 29924 examples: 0.061\n",
      "Loss/accuracy after 30000 examples: 0.143/95.720\n",
      "Train Epoch: 4 [0/7500 (0%)]\tLoss: 0.056373\n",
      "Loss after 30064 examples: 0.056\n",
      "Train Epoch: 4 [320/7500 (4%)]\tLoss: 0.028279\n",
      "Loss after 30384 examples: 0.028\n",
      "Train Epoch: 4 [640/7500 (8%)]\tLoss: 0.062821\n",
      "Loss after 30704 examples: 0.063\n",
      "Train Epoch: 4 [960/7500 (13%)]\tLoss: 0.085396\n",
      "Loss after 31024 examples: 0.085\n",
      "Train Epoch: 4 [1280/7500 (17%)]\tLoss: 0.075021\n",
      "Loss after 31344 examples: 0.075\n",
      "Train Epoch: 4 [1600/7500 (21%)]\tLoss: 0.129148\n",
      "Loss after 31664 examples: 0.129\n",
      "Train Epoch: 4 [1920/7500 (25%)]\tLoss: 0.093849\n",
      "Loss after 31984 examples: 0.094\n",
      "Train Epoch: 4 [2240/7500 (30%)]\tLoss: 0.025705\n",
      "Loss after 32304 examples: 0.026\n",
      "Train Epoch: 4 [2560/7500 (34%)]\tLoss: 0.087479\n",
      "Loss after 32624 examples: 0.087\n",
      "Train Epoch: 4 [2880/7500 (38%)]\tLoss: 0.070249\n",
      "Loss after 32944 examples: 0.070\n",
      "Train Epoch: 4 [3200/7500 (42%)]\tLoss: 0.033029\n",
      "Loss after 33264 examples: 0.033\n",
      "Train Epoch: 4 [3520/7500 (47%)]\tLoss: 0.052147\n",
      "Loss after 33584 examples: 0.052\n",
      "Train Epoch: 4 [3840/7500 (51%)]\tLoss: 0.042290\n",
      "Loss after 33904 examples: 0.042\n",
      "Train Epoch: 4 [4160/7500 (55%)]\tLoss: 0.074709\n",
      "Loss after 34224 examples: 0.075\n",
      "Train Epoch: 4 [4480/7500 (59%)]\tLoss: 0.057272\n",
      "Loss after 34544 examples: 0.057\n",
      "Train Epoch: 4 [4800/7500 (64%)]\tLoss: 0.050243\n",
      "Loss after 34864 examples: 0.050\n",
      "Train Epoch: 4 [5120/7500 (68%)]\tLoss: 0.064747\n",
      "Loss after 35184 examples: 0.065\n",
      "Train Epoch: 4 [5440/7500 (72%)]\tLoss: 0.018431\n",
      "Loss after 35504 examples: 0.018\n",
      "Train Epoch: 4 [5760/7500 (76%)]\tLoss: 0.085959\n",
      "Loss after 35824 examples: 0.086\n",
      "Train Epoch: 4 [6080/7500 (81%)]\tLoss: 0.125620\n",
      "Loss after 36144 examples: 0.126\n",
      "Train Epoch: 4 [6400/7500 (85%)]\tLoss: 0.109780\n",
      "Loss after 36464 examples: 0.110\n",
      "Train Epoch: 4 [6720/7500 (89%)]\tLoss: 0.019524\n",
      "Loss after 36784 examples: 0.020\n",
      "Train Epoch: 4 [7040/7500 (93%)]\tLoss: 0.087944\n",
      "Loss after 37104 examples: 0.088\n",
      "Train Epoch: 4 [7360/7500 (97%)]\tLoss: 0.043232\n",
      "Loss after 37424 examples: 0.043\n",
      "Loss/accuracy after 37500 examples: 0.133/95.960\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆████████</td></tr><tr><td>train/loss</td><td>█▆▄▃▂▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation/accuracy</td><td>▁▆▇██</td></tr><tr><td>validation/loss</td><td>█▄▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>0.04323</td></tr><tr><td>validation/accuracy</td><td>95.96</td></tr><tr><td>validation/loss</td><td>0.13267</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">stellar-voice-2</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/23718yto\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/23718yto</a><br/>Synced 7 W&B file(s), 0 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154514-23718yto/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154533-2gjpganp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/2gjpganp\" target=\"_blank\">sandy-sun-3</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>95.65</td></tr><tr><td>loss</td><td>0.13619</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sandy-sun-3</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/2gjpganp\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/2gjpganp</a><br/>Synced 7 W&B file(s), 32 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154533-2gjpganp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    loss, accuracy = test(model, test_loader)\n",
    "    highest_losses, hardest_examples, true_labels, predictions = get_hardest_k_examples(model, test_loader.dataset)\n",
    "    return loss, accuracy, highest_losses, hardest_examples, true_labels, predictions\n",
    "\n",
    "def get_hardest_k_examples(model, testing_set, k=32):\n",
    "    model.eval()\n",
    "    loader = DataLoader(testing_set, 1, shuffle=False)\n",
    "    # get the losses and predictions for each item in the dataset\n",
    "    losses = None\n",
    "    predictions = None\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            if losses is None:\n",
    "                losses = loss.view((1, 1))\n",
    "                predictions = pred\n",
    "            else:\n",
    "                losses = torch.cat((losses, loss.view((1, 1))), 0)\n",
    "                predictions = torch.cat((predictions, pred), 0)\n",
    "\n",
    "    argsort_loss = torch.argsort(losses, dim=0)\n",
    "    highest_k_losses = losses[argsort_loss[-k:]]\n",
    "    hardest_k_examples = testing_set[argsort_loss[-k:]][0]\n",
    "    true_labels = testing_set[argsort_loss[-k:]][1]\n",
    "    predicted_labels = predictions[argsort_loss[-k:]]\n",
    "    return highest_k_losses, hardest_k_examples, true_labels, predicted_labels\n",
    "\n",
    "def train_and_log(model_config, train_config):\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"train\", config=train_config) as run:\n",
    "        train_config = wandb.config\n",
    "        data = run.use_artifact('mnist-preprocess:latest')\n",
    "        data_dir = data.download()\n",
    "\n",
    "        training_dataset = read(data_dir, \"training\")\n",
    "        validation_dataset = read(data_dir, \"validation\")\n",
    "        train_loader = DataLoader(training_dataset, batch_size=train_config.batch_size)\n",
    "        validation_loader = DataLoader(validation_dataset, batch_size=train_config.batch_size)\n",
    "        \n",
    "        train_config.update(model_config)\n",
    "        model = ConvNet(**model_config)\n",
    "        model = model.to(device)\n",
    "        train(model, train_loader, validation_loader, train_config)\n",
    "        \n",
    "        model_artifact = wandb.Artifact(\n",
    "            \"trained-model\", type=\"model\",\n",
    "            description=\"Trained NN model\",\n",
    "            metadata=dict(model_config))\n",
    "\n",
    "        with model_artifact.new_file(\"trained_model.pth\", mode=\"wb\") as file:\n",
    "            torch.save(model.state_dict(), file)\n",
    "\n",
    "        run.log_artifact(model_artifact)\n",
    "\n",
    "    return model\n",
    "    \n",
    "def evaluate_and_log(config=None):\n",
    "    with wandb.init(project=PROJECT_NAME, job_type=\"report\", config=config) as run:\n",
    "        data = run.use_artifact('mnist-preprocess:latest')\n",
    "        data_dir = data.download()\n",
    "        testing_set = read(data_dir, \"test\")\n",
    "        test_loader = torch.utils.data.DataLoader(testing_set, batch_size=128, shuffle=False)\n",
    "\n",
    "        model_artifact = run.use_artifact(\"trained-model:latest\")\n",
    "        model_dir = model_artifact.download()\n",
    "        model_path = os.path.join(model_dir, \"trained_model.pth\")\n",
    "        model_config = model_artifact.metadata\n",
    "\n",
    "        model = ConvNet(**model_config)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.to(device)\n",
    "\n",
    "        loss, accuracy, highest_losses, hardest_examples, true_labels, preds = evaluate(model, test_loader)\n",
    "        run.summary.update({\"loss\": loss, \"accuracy\": accuracy})\n",
    "\n",
    "        wandb.log({\"high-loss-examples\":\n",
    "            [wandb.Image(hard_example, caption=str(int(pred)) + \",\" +  str(int(label)))\n",
    "                for hard_example, pred, label in zip(hardest_examples, preds, true_labels)]})\n",
    "\n",
    "def read(data_dir, ds_name):\n",
    "    filename = ds_name + \".pt\"\n",
    "    x, y = torch.load(os.path.join(data_dir, filename))\n",
    "    return TensorDataset(x, y)\n",
    "\n",
    "model_config = {\"hidden_layer_sizes\": [32, 64],\n",
    "                \"kernel_sizes\": [3],\n",
    "                \"activation\": \"ReLU\",\n",
    "                \"pool_sizes\": [2],\n",
    "                \"dropout\": 0.5,\n",
    "                \"num_classes\": 10}\n",
    "\n",
    "train_config = {\"batch_size\": 64,\n",
    "                \"epochs\": TRAIN_EPOCH,\n",
    "                \"batch_log_interval\": 5,\n",
    "                \"optimizer\": \"Adam\"}\n",
    "\n",
    "model = train_and_log(model_config, train_config)\n",
    "evaluate_and_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run.use_artifact\n",
    "\n",
    "To use an Artifact, we need to know its name and its version.\n",
    "\n",
    "By default, the last uploaded version is tagged as `latest`. Versions are separated from names with `:`, so the Artifact we want is `mnist-preprocess:latest`.\n",
    "\n",
    "<br>\n",
    "\n",
    "### artifact.download\n",
    "\n",
    "Before we actually download anything, we check to see if the right version is available locally by using *hashing*.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: the steps of the preprocessing are saved with the `preprocessed_data` as metadata.\n",
    "\n",
    "If you're trying to make your experiments reproducible, capturing lots of metadata is a good idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning\n",
    "\n",
    "In Wandb, *Hyperparameter Sweeps* provide an organized and efficient way to search through high dimensional hyperparameter spaces to find the most performant model.\n",
    "\n",
    "They enable this by automatically searching through combinations of hyperparameter values (e.g. learning rate, batch size, number of hidden layers, optimizer type) to find the most optimal values.\n",
    "\n",
    "![wandb-sweep-overview](assets/wandb-sweep-overview.png)\n",
    "\n",
    "To run a hyperparameter sweep with Wandb, there are 3 simple steps:\n",
    "\n",
    "1.  Define the sweep configuration\n",
    "\n",
    "    We do this by creating a dictionary that specifies the search strategy, optimization metric, and parameters to search through.\n",
    "\n",
    "1.  Initialize the sweep\n",
    "    \n",
    "    We initialize the sweep and pass in the dictionary of sweep configurations\n",
    "\n",
    "    ```bash\n",
    "    sweep_id = wandb.sweep(sweep_config)\n",
    "    ```\n",
    "\n",
    "1.  Run the sweep agent\n",
    "    \n",
    "    We call `wandb.agent()` and pass the `sweep_id` to run, along with a function that defines your training steps:\n",
    "\n",
    "    ```bash\n",
    "    wandb.agent(sweep_id, function=train)\n",
    "    ```\n",
    "\n",
    "In this section, we'll see how you can run sophisticated hyperparameter sweeps using Wandb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Define Sweep config\n",
    "\n",
    "A Sweep combines a strategy for trying out a bunch of hyperparameter values with the code that evalutes them.\n",
    "\n",
    "#### Pick a method\n",
    "The first thing we need to define is the method for choosing new parameter values. It can be:\n",
    "\n",
    "- `grid` Search – Iterate over every combination of hyperparameter values. Very effective, but can be computationally costly.\n",
    "- `random` Search – Select each new combination at random according to provided distributions. Surprisingly effective!\n",
    "- `bayesian` Search – Create a probabilistic model of metric score as a function of the hyperparameters, and choose parameters with high probability of improving the metric. Works well for small numbers of continuous parameters but scales poorly.\n",
    "\n",
    "We select `random` Search for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've picked a method to try out new values of the hyperparameters, you need to define what those parameters are.\n",
    "\n",
    "This step is straightforward: just give the parameter a name and specify a list of legal values of the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'random',\n",
       " 'parameters': {'epochs': {'value': 5},\n",
       "  'optimizer': {'values': ['adam', 'sgd']},\n",
       "  'hidden_layer_1_size': {'values': [16, 32, 64]},\n",
       "  'hidden_layer_2_size': {'values': [32, 64, 128]},\n",
       "  'dropout': {'values': [0.4, 0.5, 0.6]},\n",
       "  'learning_rate': {'distribution': 'uniform', 'min': 0, 'max': 0.1},\n",
       "  'batch_size': {'distribution': 'q_log_uniform_values',\n",
       "   'q': 8,\n",
       "   'min': 32,\n",
       "   'max': 256}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    # epochs var doesn't vary, but we still want it here\n",
    "    'epochs': {\n",
    "        'value': SWEEP_EPOCH,\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd'],\n",
    "    },\n",
    "    'hidden_layer_1_size': {\n",
    "        'values': [16, 32, 64],\n",
    "    },\n",
    "    'hidden_layer_2_size': {\n",
    "        'values': [32, 64, 128],\n",
    "    },\n",
    "    'dropout': {\n",
    "        'values': [0.4, 0.5, 0.6],\n",
    "    },\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0,\n",
    "        'max': 0.1\n",
    "    },\n",
    "    'batch_size': {\n",
    "        # integers between 32 and 256\n",
    "        # with evenly-distributed logarithms \n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q': 8,\n",
    "        'min': 32,\n",
    "        'max': 256,\n",
    "    }\n",
    "}\n",
    "sweep_config['parameters'] = parameters\n",
    "sweep_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wandb also offers the option to `early_terminate` your runs with the `HyperBand` scheduling algorithm. See more [here](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#early_terminate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Run the Sweep\n",
    "\n",
    "The Sweep Controller is in charge of our Sweep. The Sweep Controller instructs how to run each set of hyperparameters via Agents.\n",
    "\n",
    "In a typical Sweep, the Controller lives on Wandb's server, while the agents who complete runs live on your machine(s). This makes it easy to scale up Sweeps by just adding more machines to run agents!\n",
    "\n",
    "![sweeps-diagram](assets/sweeps-diagram.png)\n",
    "\n",
    "We can initialize a Sweep Controller by calling `wandb.sweep` with `sweep_config` and project name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 0bncib9q\n",
      "Sweep URL: http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can actually execute the sweep, we need to define the training procedure used in the Sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_log_interval = 5\n",
    "\n",
    "def train_sweep(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        loader = build_dataset(run, config)\n",
    "        model = build_model(run, config)\n",
    "        optimizer = build_optimizer(model, config)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            train_epoch(model, loader, None, batch_log_interval, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines: `build_dataset`, `build_model`, and `build_optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(run, config):\n",
    "    batch_size = config.batch_size\n",
    "    data = run.use_artifact('mnist-preprocess:latest')\n",
    "    data_dir = data.download()\n",
    "    training_dataset = read(data_dir, \"training\")\n",
    "    sub_dataset = torch.utils.data.Subset(\n",
    "        training_dataset, indices=range(0, len(training_dataset), SWEEP_N_TRAIN))\n",
    "    train_loader = DataLoader(sub_dataset, batch_size=batch_size)\n",
    "    return train_loader\n",
    "\n",
    "def build_model(run, config):\n",
    "    model_config = {\n",
    "        'hidden_layer_sizes': [\n",
    "            config.hidden_layer_1_size,\n",
    "            config.hidden_layer_2_size,\n",
    "        ],\n",
    "        'dropout': config.dropout,\n",
    "    }\n",
    "    model = ConvNet(**model_config)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "        \n",
    "def build_optimizer(model, config):\n",
    "    optimizer = config.optimizer\n",
    "    learning_rate = config.learning_rate\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will launch an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: se3ju6kz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.09890097306893914\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154545-se3ju6kz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/se3ju6kz\" target=\"_blank\">lyric-sweep-1</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.321326\n",
      "Loss after 00075 examples: 2.321\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 2.228959\n",
      "Loss after 00150 examples: 2.229\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.137083\n",
      "Loss after 00225 examples: 2.137\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.048993\n",
      "Loss after 00300 examples: 2.049\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 1.861895\n",
      "Loss after 00375 examples: 1.862\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f4756d1927411db2fc70873e9b43db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.142 MB of 0.142 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>█▇▅▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>1.86189</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lyric-sweep-1</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/se3ju6kz\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/se3ju6kz</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154545-se3ju6kz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ajggrkkm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.023040291197619436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154558-ajggrkkm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/ajggrkkm\" target=\"_blank\">efficient-sweep-2</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.315013\n",
      "Loss after 00075 examples: 2.315\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 6.121810\n",
      "Loss after 00150 examples: 6.122\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 3.006755\n",
      "Loss after 00225 examples: 3.007\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.281423\n",
      "Loss after 00300 examples: 2.281\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.279048\n",
      "Loss after 00375 examples: 2.279\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1083da9674b48099fe23070007b9a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.142 MB of 0.142 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>▁█▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.27905</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">efficient-sweep-2</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/ajggrkkm\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/ajggrkkm</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154558-ajggrkkm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zchyjf7e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.06792388906019743\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154609-zchyjf7e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/zchyjf7e\" target=\"_blank\">driven-sweep-3</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.313268\n",
      "Loss after 00075 examples: 2.313\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 44.499847\n",
      "Loss after 00150 examples: 44.500\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 8.937371\n",
      "Loss after 00225 examples: 8.937\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.638805\n",
      "Loss after 00300 examples: 2.639\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.208674\n",
      "Loss after 00375 examples: 2.209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c09d1d1e97048e197f183eeca61dcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.142 MB of 0.142 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>▁█▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.20867</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">driven-sweep-3</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/zchyjf7e\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/zchyjf7e</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154609-zchyjf7e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cz5ctzhb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.03801925910046388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154620-cz5ctzhb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/cz5ctzhb\" target=\"_blank\">dark-sweep-4</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.312479\n",
      "Loss after 00040 examples: 2.312\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 7.565046\n",
      "Loss after 00115 examples: 7.565\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.326441\n",
      "Loss after 00190 examples: 2.326\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.263224\n",
      "Loss after 00265 examples: 2.263\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.212342\n",
      "Loss after 00340 examples: 2.212\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d054094729146c686c68d68df861041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.142 MB of 0.142 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>▁█▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.21234</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dark-sweep-4</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/cz5ctzhb\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/cz5ctzhb</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154620-cz5ctzhb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 81kdham9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.09293435729933248\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154634-81kdham9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/81kdham9\" target=\"_blank\">cosmic-sweep-5</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.286695\n",
      "Loss after 00075 examples: 2.287\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 2.273851\n",
      "Loss after 00150 examples: 2.274\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.214229\n",
      "Loss after 00225 examples: 2.214\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.173522\n",
      "Loss after 00300 examples: 2.174\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.084430\n",
      "Loss after 00375 examples: 2.084\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc79d734baf24489b7e5db2b5ca797af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.142 MB of 0.142 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>██▅▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.08443</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cosmic-sweep-5</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/81kdham9\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/81kdham9</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154634-81kdham9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oxk25f8x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 88\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.028019561749959922\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154645-oxk25f8x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/oxk25f8x\" target=\"_blank\">rose-sweep-6</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.317863\n",
      "Loss after 00075 examples: 2.318\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 2.265687\n",
      "Loss after 00150 examples: 2.266\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.214191\n",
      "Loss after 00225 examples: 2.214\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.202099\n",
      "Loss after 00300 examples: 2.202\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.115085\n",
      "Loss after 00375 examples: 2.115\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7919efd65c704a16b84914b8501c54a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.142 MB of 0.142 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>█▆▄▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.11508</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">rose-sweep-6</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/oxk25f8x\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/oxk25f8x</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154645-oxk25f8x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 81835t3o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.07322103686125231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154658-81835t3o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/81835t3o\" target=\"_blank\">dulcet-sweep-7</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.300242\n",
      "Loss after 00075 examples: 2.300\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 2.276975\n",
      "Loss after 00150 examples: 2.277\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.197116\n",
      "Loss after 00225 examples: 2.197\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.129936\n",
      "Loss after 00300 examples: 2.130\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.026690\n",
      "Loss after 00375 examples: 2.027\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0eb48bf7db94cd4a36183a519a0be9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.067 MB of 0.067 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>█▇▅▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.02669</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dulcet-sweep-7</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/81835t3o\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/81835t3o</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154658-81835t3o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xekf0j5i with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.019269119359063648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154709-xekf0j5i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/xekf0j5i\" target=\"_blank\">clear-sweep-8</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.299385\n",
      "Loss after 00075 examples: 2.299\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 2.305153\n",
      "Loss after 00150 examples: 2.305\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.281292\n",
      "Loss after 00225 examples: 2.281\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.241487\n",
      "Loss after 00300 examples: 2.241\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.220551\n",
      "Loss after 00375 examples: 2.221\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6ed6ac61fb4528a45bb065326bc86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.067 MB of 0.067 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>██▆▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.22055</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">clear-sweep-8</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/xekf0j5i\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/xekf0j5i</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154709-xekf0j5i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: v8hrswzw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.046045378555942096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154720-v8hrswzw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/v8hrswzw\" target=\"_blank\">atomic-sweep-9</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.313143\n",
      "Loss after 00075 examples: 2.313\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 2.275455\n",
      "Loss after 00150 examples: 2.275\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.195082\n",
      "Loss after 00225 examples: 2.195\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.156192\n",
      "Loss after 00300 examples: 2.156\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.091573\n",
      "Loss after 00375 examples: 2.092\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba252903848b432db885ec2ced44b988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.067 MB of 0.067 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>█▇▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.09157</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">atomic-sweep-9</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/v8hrswzw\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/v8hrswzw</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154720-v8hrswzw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cz018xu5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.06126633103631588\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154731-cz018xu5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/cz018xu5\" target=\"_blank\">sunny-sweep-10</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.308008\n",
      "Loss after 00075 examples: 2.308\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 30.347223\n",
      "Loss after 00150 examples: 30.347\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 6.552475\n",
      "Loss after 00225 examples: 6.552\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.633225\n",
      "Loss after 00300 examples: 2.633\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.129053\n",
      "Loss after 00375 examples: 2.129\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d70667fdd4a4e9dafecfd1b2410b90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.067 MB of 0.067 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>▁█▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.12905</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">sunny-sweep-10</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/cz018xu5\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/cz018xu5</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154731-cz018xu5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ck875xgb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.012301603580304766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154744-ck875xgb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/ck875xgb\" target=\"_blank\">swept-sweep-11</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.290488\n",
      "Loss after 00075 examples: 2.290\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 2.308425\n",
      "Loss after 00150 examples: 2.308\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.291225\n",
      "Loss after 00225 examples: 2.291\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.278205\n",
      "Loss after 00300 examples: 2.278\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.275478\n",
      "Loss after 00375 examples: 2.275\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c527f22053448c8b6c4caccefaf0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.067 MB of 0.067 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>▄█▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.27548</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">swept-sweep-11</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/ck875xgb\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/ck875xgb</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154744-ck875xgb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6x9cruhd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.022699092471181206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154754-6x9cruhd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/6x9cruhd\" target=\"_blank\">dry-sweep-12</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.315115\n",
      "Loss after 00064 examples: 2.315\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 8.974828\n",
      "Loss after 00139 examples: 8.975\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.614850\n",
      "Loss after 00214 examples: 2.615\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.242105\n",
      "Loss after 00289 examples: 2.242\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.231936\n",
      "Loss after 00364 examples: 2.232\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4657c75292446a91f1b912c8d6742f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.067 MB of 0.067 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>▁█▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.23194</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dry-sweep-12</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/6x9cruhd\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/6x9cruhd</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154754-6x9cruhd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2our8n14 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01806892261971368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154805-2our8n14</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/2our8n14\" target=\"_blank\">fresh-sweep-13</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.308710\n",
      "Loss after 00064 examples: 2.309\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 2.329162\n",
      "Loss after 00139 examples: 2.329\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.282286\n",
      "Loss after 00214 examples: 2.282\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.217520\n",
      "Loss after 00289 examples: 2.218\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.196024\n",
      "Loss after 00364 examples: 2.196\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a38917dace45c2bbab559402c4e2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.067 MB of 0.067 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>▇█▆▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.19602</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fresh-sweep-13</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/2our8n14\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/2our8n14</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154805-2our8n14/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fw7j7yab with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 232\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0760814698250444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154819-fw7j7yab</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/fw7j7yab\" target=\"_blank\">vocal-sweep-14</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.298564\n",
      "Loss after 00075 examples: 2.299\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 2.258331\n",
      "Loss after 00150 examples: 2.258\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.195745\n",
      "Loss after 00225 examples: 2.196\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.145136\n",
      "Loss after 00300 examples: 2.145\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.038334\n",
      "Loss after 00375 examples: 2.038\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e4e1af40dd4ae1af8e0d7fe8a369f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.067 MB of 0.067 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>█▇▅▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.03833</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vocal-sweep-14</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/fw7j7yab\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/fw7j7yab</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154819-fw7j7yab/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cct2nj8p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_1_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_2_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.008146352658083767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tung.dao/tung/soict/code/soict-2022/tutorial/wandb/run-20221121_154830-cct2nj8p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/cct2nj8p\" target=\"_blank\">polar-sweep-15</a></strong> to <a href=\"http://localhost:8080/demo/soict-2022-heavy\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/sweeps/0bncib9q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/75 (0%)]\tLoss: 2.299235\n",
      "Loss after 00032 examples: 2.299\n",
      "Train Epoch: 1 [0/75 (0%)]\tLoss: 2.615812\n",
      "Loss after 00107 examples: 2.616\n",
      "Train Epoch: 2 [0/75 (0%)]\tLoss: 2.240662\n",
      "Loss after 00182 examples: 2.241\n",
      "Train Epoch: 3 [0/75 (0%)]\tLoss: 2.214367\n",
      "Loss after 00257 examples: 2.214\n",
      "Train Epoch: 4 [0/75 (0%)]\tLoss: 2.142469\n",
      "Loss after 00332 examples: 2.142\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a37edbbad7542469ff71a02d15c4d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.067 MB of 0.067 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>▃█▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>2.14247</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">polar-sweep-15</strong>: <a href=\"http://localhost:8080/demo/soict-2022-heavy/runs/cct2nj8p\" target=\"_blank\">http://localhost:8080/demo/soict-2022-heavy/runs/cct2nj8p</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221121_154830-cct2nj8p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train_sweep, count=SWEEP_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Visualize Sweep Results\n",
    "\n",
    "#### Parallel Coordinates Plot\n",
    "\n",
    "This plot maps hyperparameter values to model metrics.\n",
    "\n",
    "![hyperparam-plot](assets/hyperparam-plot.png)\n",
    "\n",
    "#### Hyperparameter Importance Plot\n",
    "\n",
    "The hyperparameter importance plot surfaces which hyperparameters were the best predictors of your metrics. Wandb reports feature importance (using a random forest model) and correlation (using a linear model).\n",
    "\n",
    "![hyperparam-importance](assets/hyperparam-importance.png)\n",
    "\n",
    "These visualizations can help you save both time and resources running expensive hyperparameter optimizations by refining the parameters (and value ranges), and thereby worthy of further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Stop the Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Stopping sweep demo/soict-2022-heavy/0bncib9q.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Done.\n"
     ]
    }
   ],
   "source": [
    "# For self-hosted Wandb server\n",
    "!wandb sweep --stop \"demo/$PROJECT_NAME/$sweep_id\"\n",
    "\n",
    "# For Wandb cloud server\n",
    "# Stop sweep at https://<wandb-server-address>/<wandb-user>/soict-2022/sweeps/<sweep-id>/controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('soict')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "291bacc89d47c570d681c8cb8133df5aeea2ebc003da29abf8c072691acbbc14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
